{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6472\\3705838554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Calculate dense optical flow by Farneback method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcOpticalFlowFarneback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpyr_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_sigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Compute the magnitude and angle of the 2D vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mmagnitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartToPolar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Get a VideoCapture object from video and store it in vs\n",
    "vc = cv2.VideoCapture(\"April_09_brush_hair_u_nm_np1_ba_goo_0.avi\")\n",
    "# Read first frame\n",
    "ret, first_frame = vc.read()\n",
    "# Scale and resize image\n",
    "resize_dim = 600\n",
    "max_dim = max(first_frame.shape)\n",
    "scale = resize_dim/max_dim\n",
    "first_frame = cv2.resize(first_frame, None, fx=scale, fy=scale)\n",
    "# Convert to gray scale \n",
    "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Create mask\n",
    "mask = np.zeros_like(first_frame)\n",
    "# Sets image saturation to maximum\n",
    "mask[..., 1] = 255\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('video.avi',-1,1,(600, 600))\n",
    "\n",
    "while(vc.isOpened()):\n",
    "    # Read a frame from video\n",
    "    ret, frame = vc.read()\n",
    "    cv2.imshow('porcoddio', frame)\n",
    "    \n",
    "    # Convert new frame format`s to gray scale and resize gray frame obtained\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, None, fx=scale, fy=scale)\n",
    "\n",
    "    # Calculate dense optical flow by Farneback method\n",
    "    # https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, pyr_scale = 0.5, levels = 5, winsize = 11, iterations = 5, poly_n = 5, poly_sigma = 1.1, flags = 0)\n",
    "    # Compute the magnitude and angle of the 2D vectors\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Set image hue according to the optical flow direction\n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "    # Set image value according to the optical flow magnitude (normalized)\n",
    "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Convert HSV to RGB (BGR) color representation\n",
    "    cv2.imshow('porcoddio1', mask)\n",
    "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Resize frame size to match dimensions\n",
    "    frame = cv2.resize(frame, None, fx=scale, fy=scale)\n",
    "    \n",
    "    # Open a new window and displays the output frame\n",
    "    dense_flow = cv2.addWeighted(frame, 1,rgb, 2, 0)\n",
    "    cv2.imshow(\"Dense optical flow\", dense_flow)\n",
    "    out.write(dense_flow)\n",
    "    # Update previous frame\n",
    "    prev_gray = gray\n",
    "    # Frame are read by intervals of 1 millisecond. The programs breaks out of the while loop when the user presses the 'q' key\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(cv.samples.findFile(\"vtest.avi\"))\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('frame2', bgr)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv.imwrite('opticalfb.png', frame2)\n",
    "        cv.imwrite('opticalhsv.png', bgr)\n",
    "    prvs = next\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dense_optical_flow(method, video_path, params=[], to_gray=False):\n",
    "    # read the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Read the first frame\n",
    "    ret, old_frame = cap.read()\n",
    "\n",
    "    # crate HSV & make Value a constant\n",
    "    hsv = np.zeros_like(old_frame)\n",
    "    hsv[..., 1] = 255\n",
    "\n",
    "    # Preprocessing for exact method\n",
    "    if to_gray:\n",
    "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, new_frame = cap.read()\n",
    "        frame_copy = new_frame\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocessing for exact method\n",
    "        if to_gray:\n",
    "            new_frame = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Calculate Optical Flow\n",
    "        flow = method(old_frame, new_frame, None, *params)\n",
    "\n",
    "        # Encoding: convert the algorithm's output into Polar coordinates\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        # Use Hue and Saturation to encode the Optical Flow\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        # Convert HSV image into BGR for demo\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        cv2.imshow(\"frame\", frame_copy)\n",
    "        cv2.imshow(\"optical flow\", bgr)\n",
    "        k = cv2.waitKey(25) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "        old_frame = new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lucas_kanade_method(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # params for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "    # Parameters for lucas kanade optical flow\n",
    "    lk_params = dict(\n",
    "        winSize=(15, 15),\n",
    "        maxLevel=2,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "    )\n",
    "    # Create some random colors\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "    # Take first frame and find corners in it\n",
    "    ret, old_frame = cap.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(old_frame)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # calculate optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "            old_gray, frame_gray, p0, None, **lk_params\n",
    "        )\n",
    "        # Select good points\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "        # draw the tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "            frame = cv2.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "        img = cv2.add(frame, mask)\n",
    "        cv2.imshow(\"frame\", img)\n",
    "        k = cv2.waitKey(25) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "        if k == ord(\"c\"):\n",
    "            mask = np.zeros_like(old_frame)\n",
    "        # Now update the previous frame and previous points\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"lucaskanade\":\n",
    "video_path = \"Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\n",
    "lucas_kanade_method(video_path)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "video_path = \"Stabilized/Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\n",
    "lucas_kanade_method(video_path)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lucaskanade_dense\n",
    "method = cv2.optflow.calcOpticalFlowSparseToDense\n",
    "dense_optical_flow(method, \"April_09_brush_hair_u_nm_np1_ba_goo_0.avi\", to_gray=True)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "method = cv2.optflow.calcOpticalFlowSparseToDense\n",
    "dense_optical_flow(method, \"Stabilized/Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\", to_gray=True)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26128\\2418424541.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcOpticalFlowFarneback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Farneback's algorithm parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdense_optical_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_gray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26128\\1043914491.py\u001b[0m in \u001b[0;36mdense_optical_flow\u001b[1;34m(method, video_path, params, to_gray)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optical flow\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_path = \"April_09_brush_hair_u_nm_np1_ba_goo_0.avi\"\n",
    "method = cv2.calcOpticalFlowFarneback\n",
    "params = [0.5, 3, 15, 3, 5, 1.2, 0]  # Farneback's algorithm parameters\n",
    "dense_optical_flow(method, video_path, params, to_gray=True)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "video_path = \"Stabilized/Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\n",
    "method = cv2.calcOpticalFlowFarneback\n",
    "params = [0.5, 3, 15, 3, 5, 1.2, 0]  # Farneback's algorithm parameters\n",
    "dense_optical_flow(method, video_path, params, to_gray=True)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34664\\354964527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Stabilized/Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcOpticalFlowDenseRLOF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdense_optical_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34664\\1043914491.py\u001b[0m in \u001b[0;36mdense_optical_flow\u001b[1;34m(method, video_path, params, to_gray)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mnew_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Calculate Optical Flow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Encoding: convert the algorithm's output into Polar coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#RLOF\n",
    "video_path = \"Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\n",
    "method = cv2.optflow.calcOpticalFlowDenseRLOF\n",
    "dense_optical_flow(method, video_path)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "video_path = \"Stabilized/Increase_stride_length_by_running_stairs__working_step_over_action_speed_development_climb_stairs_f_cm_np1_le_med_1.avi\"\n",
    "method = cv2.optflow.calcOpticalFlowDenseRLOF\n",
    "dense_optical_flow(method, video_path)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module cv2.optflow in cv2:\n",
      "\n",
      "NAME\n",
      "    cv2.optflow\n",
      "\n",
      "FUNCTIONS\n",
      "    DenseRLOFOpticalFlow_create(...)\n",
      "        DenseRLOFOpticalFlow_create([, rlofParam[, forwardBackwardThreshold[, gridStep[, interp_type[, epicK[, epicSigma[, epicLambda[, ricSPSize[, ricSLICType[, use_post_proc[, fgsLambda[, fgsSigma[, use_variational_refinement]]]]]]]]]]]]]) -> retval\n",
      "        .   *    @param rlofParam see optflow::RLOFOpticalFlowParameter\n",
      "        .        *    @param forwardBackwardThreshold see setForwardBackward\n",
      "        .        *    @param gridStep see setGridStep\n",
      "        .        *    @param interp_type see setInterpolation\n",
      "        .        *    @param epicK see setEPICK\n",
      "        .        *    @param epicSigma see setEPICSigma\n",
      "        .        *    @param epicLambda see setEPICLambda\n",
      "        .        *    @param ricSPSize see setRICSPSize\n",
      "        .        *    @param ricSLICType see setRICSLICType\n",
      "        .        *    @param use_post_proc see setUsePostProc\n",
      "        .        *    @param fgsLambda see setFgsLambda\n",
      "        .        *    @param fgsSigma see setFgsSigma\n",
      "        .        *    @param use_variational_refinement see setUseVariationalRefinement\n",
      "    \n",
      "    DualTVL1OpticalFlow_create(...)\n",
      "        DualTVL1OpticalFlow_create([, tau[, lambda[, theta[, nscales[, warps[, epsilon[, innnerIterations[, outerIterations[, scaleStep[, gamma[, medianFiltering[, useInitialFlow]]]]]]]]]]]]) -> retval\n",
      "        .   @brief Creates instance of cv::DualTVL1OpticalFlow\n",
      "    \n",
      "    RLOFOpticalFlowParameter_create(...)\n",
      "        RLOFOpticalFlowParameter_create() -> retval\n",
      "        .\n",
      "    \n",
      "    SparseRLOFOpticalFlow_create(...)\n",
      "        SparseRLOFOpticalFlow_create([, rlofParam[, forwardBackwardThreshold]]) -> retval\n",
      "        .   *    @param rlofParam see setRLOFOpticalFlowParameter\n",
      "        .        *    @param forwardBackwardThreshold see setForwardBackward\n",
      "    \n",
      "    calcOpticalFlowDenseRLOF(...)\n",
      "        calcOpticalFlowDenseRLOF(I0, I1, flow[, rlofParam[, forwardBackwardThreshold[, gridStep[, interp_type[, epicK[, epicSigma[, epicLambda[, ricSPSize[, ricSLICType[, use_post_proc[, fgsLambda[, fgsSigma[, use_variational_refinement]]]]]]]]]]]]]) -> flow\n",
      "        .   @brief Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme.\n",
      "        .    *\n",
      "        .    * The RLOF is a fast local optical flow approach described in @cite Senst2012 @cite Senst2013 @cite Senst2014\n",
      "        .    * and @cite Senst2016 similar to the pyramidal iterative Lucas-Kanade method as\n",
      "        .    * proposed by @cite Bouguet00. More details and experiments can be found in the following thesis @cite Senst2019.\n",
      "        .    * The implementation is derived from optflow::calcOpticalFlowPyrLK().\n",
      "        .    *\n",
      "        .    * The sparse-to-dense interpolation scheme allows for fast computation of dense optical flow using RLOF (see @cite Geistert2016).\n",
      "        .    * For this scheme the following steps are applied:\n",
      "        .    * -# motion vector seeded at a regular sampled grid are computed. The sparsity of this grid can be configured with setGridStep\n",
      "        .    * -# (optinally) errornous motion vectors are filter based on the forward backward confidence. The threshold can be configured\n",
      "        .    * with setForwardBackward. The filter is only applied if the threshold >0 but than the runtime is doubled due to the estimation\n",
      "        .    * of the backward flow.\n",
      "        .    * -# Vector field interpolation is applied to the motion vector set to obtain a dense vector field.\n",
      "        .    *\n",
      "        .    * @param I0 first 8-bit input image. If The cross-based RLOF is used (by selecting optflow::RLOFOpticalFlowParameter::supportRegionType\n",
      "        .    * = SupportRegionType::SR_CROSS) image has to be a 8-bit 3 channel image.\n",
      "        .    * @param I1 second 8-bit input image. If The cross-based RLOF is used (by selecting optflow::RLOFOpticalFlowParameter::supportRegionType\n",
      "        .    * = SupportRegionType::SR_CROSS) image has to be a 8-bit 3 channel image.\n",
      "        .    * @param flow computed flow image that has the same size as I0 and type CV_32FC2.\n",
      "        .    * @param rlofParam see optflow::RLOFOpticalFlowParameter\n",
      "        .    * @param forwardBackwardThreshold Threshold for the forward backward confidence check.\n",
      "        .    * For each grid point \\f$ \\mathbf{x} \\f$ a motion vector \\f$ d_{I0,I1}(\\mathbf{x}) \\f$ is computed.\n",
      "        .    * If the forward backward error \\f[ EP_{FB} = || d_{I0,I1} + d_{I1,I0} || \\f]\n",
      "        .    * is larger than threshold given by this function then the motion vector will not be used by the following\n",
      "        .    * vector field interpolation. \\f$ d_{I1,I0} \\f$ denotes the backward flow. Note, the forward backward test\n",
      "        .    *    will only be applied if the threshold > 0. This may results into a doubled runtime for the motion estimation.\n",
      "        .    * @param gridStep Size of the grid to spawn the motion vectors. For each grid point a motion vector is computed.\n",
      "        .    * Some motion vectors will be removed due to the forwatd backward threshold (if set >0). The rest will be the\n",
      "        .    * base of the vector field interpolation.\n",
      "        .    * @param interp_type interpolation method used to compute the dense optical flow. Two interpolation algorithms are\n",
      "        .    * supported:\n",
      "        .    * - **INTERP_GEO** applies the fast geodesic interpolation, see @cite Geistert2016.\n",
      "        .    * - **INTERP_EPIC_RESIDUAL** applies the edge-preserving interpolation, see @cite Revaud2015,Geistert2016.\n",
      "        .    * @param epicK see ximgproc::EdgeAwareInterpolator sets the respective parameter.\n",
      "        .    * @param epicSigma see ximgproc::EdgeAwareInterpolator sets the respective parameter.\n",
      "        .    * @param epicLambda see ximgproc::EdgeAwareInterpolator sets the respective parameter.\n",
      "        .    * @param ricSPSize  see ximgproc::RICInterpolator sets the respective parameter.\n",
      "        .    * @param ricSLICType see ximgproc::RICInterpolator sets the respective parameter.\n",
      "        .    * @param use_post_proc enables ximgproc::fastGlobalSmootherFilter() parameter.\n",
      "        .    * @param fgsLambda sets the respective ximgproc::fastGlobalSmootherFilter() parameter.\n",
      "        .    * @param fgsSigma sets the respective ximgproc::fastGlobalSmootherFilter() parameter.\n",
      "        .    * @param use_variational_refinement enables VariationalRefinement\n",
      "        .    *\n",
      "        .    * Parameters have been described in @cite Senst2012, @cite Senst2013, @cite Senst2014, @cite Senst2016.\n",
      "        .    * For the RLOF configuration see optflow::RLOFOpticalFlowParameter for further details.\n",
      "        .    * @note If the grid size is set to (1,1) and the forward backward threshold <= 0 that the dense optical flow field is purely\n",
      "        .    * computed with the RLOF.\n",
      "        .    *\n",
      "        .    * @note SIMD parallelization is only available when compiling with SSE4.1.\n",
      "        .    *\n",
      "        .    * @sa optflow::DenseRLOFOpticalFlow, optflow::RLOFOpticalFlowParameter\n",
      "    \n",
      "    calcOpticalFlowSF(...)\n",
      "        calcOpticalFlowSF(from, to, layers, averaging_block_size, max_flow[, flow]) -> flow\n",
      "        .   @overload\n",
      "        \n",
      "        \n",
      "        \n",
      "        calcOpticalFlowSF(from, to, layers, averaging_block_size, max_flow, sigma_dist, sigma_color, postprocess_window, sigma_dist_fix, sigma_color_fix, occ_thr, upscale_averaging_radius, upscale_sigma_dist, upscale_sigma_color, speed_up_thr[, flow]) -> flow\n",
      "        .   @brief Calculate an optical flow using \"SimpleFlow\" algorithm.\n",
      "        .   \n",
      "        .   @param from First 8-bit 3-channel image.\n",
      "        .   @param to Second 8-bit 3-channel image of the same size as prev\n",
      "        .   @param flow computed flow image that has the same size as prev and type CV_32FC2\n",
      "        .   @param layers Number of layers\n",
      "        .   @param averaging_block_size Size of block through which we sum up when calculate cost function\n",
      "        .   for pixel\n",
      "        .   @param max_flow maximal flow that we search at each level\n",
      "        .   @param sigma_dist vector smooth spatial sigma parameter\n",
      "        .   @param sigma_color vector smooth color sigma parameter\n",
      "        .   @param postprocess_window window size for postprocess cross bilateral filter\n",
      "        .   @param sigma_dist_fix spatial sigma for postprocess cross bilateralf filter\n",
      "        .   @param sigma_color_fix color sigma for postprocess cross bilateral filter\n",
      "        .   @param occ_thr threshold for detecting occlusions\n",
      "        .   @param upscale_averaging_radius window size for bilateral upscale operation\n",
      "        .   @param upscale_sigma_dist spatial sigma for bilateral upscale operation\n",
      "        .   @param upscale_sigma_color color sigma for bilateral upscale operation\n",
      "        .   @param speed_up_thr threshold to detect point with irregular flow - where flow should be\n",
      "        .   recalculated after upscale\n",
      "        .   \n",
      "        .   See @cite Tao2012 . And site of project - <http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/>.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      -   An example using the simpleFlow algorithm can be found at samples/simpleflow_demo.cpp\n",
      "    \n",
      "    calcOpticalFlowSparseRLOF(...)\n",
      "        calcOpticalFlowSparseRLOF(prevImg, nextImg, prevPts, nextPts[, status[, err[, rlofParam[, forwardBackwardThreshold]]]]) -> nextPts, status, err\n",
      "        .   @brief Calculates fast optical flow for a sparse feature set using the robust local optical flow (RLOF) similar\n",
      "        .   * to optflow::calcOpticalFlowPyrLK().\n",
      "        .   *\n",
      "        .   * The RLOF is a fast local optical flow approach described in @cite Senst2012 @cite Senst2013 @cite Senst2014\n",
      "        .    * and @cite Senst2016 similar to the pyramidal iterative Lucas-Kanade method as\n",
      "        .   * proposed by @cite Bouguet00. More details and experiments can be found in the following thesis @cite Senst2019.\n",
      "        .   * The implementation is derived from optflow::calcOpticalFlowPyrLK().\n",
      "        .   *\n",
      "        .   * @param prevImg first 8-bit input image. If The cross-based RLOF is used (by selecting optflow::RLOFOpticalFlowParameter::supportRegionType\n",
      "        .   * = SupportRegionType::SR_CROSS) image has to be a 8-bit 3 channel image.\n",
      "        .   * @param nextImg second 8-bit input image. If The cross-based RLOF is used (by selecting optflow::RLOFOpticalFlowParameter::supportRegionType\n",
      "        .   * = SupportRegionType::SR_CROSS) image has to be a 8-bit 3 channel image.\n",
      "        .   * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be single-precision\n",
      "        .   * floating-point numbers.\n",
      "        .   * @param nextPts output vector of 2D points (with single-precision floating-point coordinates) containing the calculated\n",
      "        .   * new positions of input features in the second image; when optflow::RLOFOpticalFlowParameter::useInitialFlow variable is true  the vector must\n",
      "        .   * have the same size as in the input and contain the initialization point correspondences.\n",
      "        .   * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the\n",
      "        .   * corresponding features has passed the forward backward check.\n",
      "        .   * @param err output vector of errors; each element of the vector is set to the forward backward error for the corresponding feature.\n",
      "        .   * @param rlofParam see optflow::RLOFOpticalFlowParameter\n",
      "        .   * @param forwardBackwardThreshold Threshold for the forward backward confidence check. If forewardBackwardThreshold <=0 the forward\n",
      "        .   *\n",
      "        .   * @note SIMD parallelization is only available when compiling with SSE4.1.\n",
      "        .   *\n",
      "        .   * Parameters have been described in @cite Senst2012, @cite Senst2013, @cite Senst2014 and @cite Senst2016.\n",
      "        .   * For the RLOF configuration see optflow::RLOFOpticalFlowParameter for further details.\n",
      "    \n",
      "    calcOpticalFlowSparseToDense(...)\n",
      "        calcOpticalFlowSparseToDense(from, to[, flow[, grid_step[, k[, sigma[, use_post_proc[, fgs_lambda[, fgs_sigma]]]]]]]) -> flow\n",
      "        .   @brief Fast dense optical flow based on PyrLK sparse matches interpolation.\n",
      "        .   \n",
      "        .   @param from first 8-bit 3-channel or 1-channel image.\n",
      "        .   @param to  second 8-bit 3-channel or 1-channel image of the same size as from\n",
      "        .   @param flow computed flow image that has the same size as from and CV_32FC2 type\n",
      "        .   @param grid_step stride used in sparse match computation. Lower values usually\n",
      "        .          result in higher quality but slow down the algorithm.\n",
      "        .   @param k number of nearest-neighbor matches considered, when fitting a locally affine\n",
      "        .          model. Lower values can make the algorithm noticeably faster at the cost of\n",
      "        .          some quality degradation.\n",
      "        .   @param sigma parameter defining how fast the weights decrease in the locally-weighted affine\n",
      "        .          fitting. Higher values can help preserve fine details, lower values can help to get rid\n",
      "        .          of the noise in the output flow.\n",
      "        .   @param use_post_proc defines whether the ximgproc::fastGlobalSmootherFilter() is used\n",
      "        .          for post-processing after interpolation\n",
      "        .   @param fgs_lambda see the respective parameter of the ximgproc::fastGlobalSmootherFilter()\n",
      "        .   @param fgs_sigma  see the respective parameter of the ximgproc::fastGlobalSmootherFilter()\n",
      "    \n",
      "    createOptFlow_DeepFlow(...)\n",
      "        createOptFlow_DeepFlow() -> retval\n",
      "        .   @brief DeepFlow optical flow algorithm implementation.\n",
      "        .   \n",
      "        .   The class implements the DeepFlow optical flow algorithm described in @cite Weinzaepfel2013 . See\n",
      "        .   also <http://lear.inrialpes.fr/src/deepmatching/> .\n",
      "        .   Parameters - class fields - that may be modified after creating a class instance:\n",
      "        .   -   member float alpha\n",
      "        .   Smoothness assumption weight\n",
      "        .   -   member float delta\n",
      "        .   Color constancy assumption weight\n",
      "        .   -   member float gamma\n",
      "        .   Gradient constancy weight\n",
      "        .   -   member float sigma\n",
      "        .   Gaussian smoothing parameter\n",
      "        .   -   member int minSize\n",
      "        .   Minimal dimension of an image in the pyramid (next, smaller images in the pyramid are generated\n",
      "        .   until one of the dimensions reaches this size)\n",
      "        .   -   member float downscaleFactor\n",
      "        .   Scaling factor in the image pyramid (must be \\< 1)\n",
      "        .   -   member int fixedPointIterations\n",
      "        .   How many iterations on each level of the pyramid\n",
      "        .   -   member int sorIterations\n",
      "        .   Iterations of Succesive Over-Relaxation (solver)\n",
      "        .   -   member float omega\n",
      "        .   Relaxation factor in SOR\n",
      "    \n",
      "    createOptFlow_DenseRLOF(...)\n",
      "        createOptFlow_DenseRLOF() -> retval\n",
      "        .\n",
      "    \n",
      "    createOptFlow_DualTVL1(...)\n",
      "        createOptFlow_DualTVL1() -> retval\n",
      "        .   @brief Creates instance of cv::DenseOpticalFlow\n",
      "    \n",
      "    createOptFlow_Farneback(...)\n",
      "        createOptFlow_Farneback() -> retval\n",
      "        .\n",
      "    \n",
      "    createOptFlow_PCAFlow(...)\n",
      "        createOptFlow_PCAFlow() -> retval\n",
      "        .   @brief Creates an instance of PCAFlow\n",
      "    \n",
      "    createOptFlow_SimpleFlow(...)\n",
      "        createOptFlow_SimpleFlow() -> retval\n",
      "        .\n",
      "    \n",
      "    createOptFlow_SparseRLOF(...)\n",
      "        createOptFlow_SparseRLOF() -> retval\n",
      "        .\n",
      "    \n",
      "    createOptFlow_SparseToDense(...)\n",
      "        createOptFlow_SparseToDense() -> retval\n",
      "        .\n",
      "\n",
      "DATA\n",
      "    GPC_DESCRIPTOR_DCT = 0\n",
      "    GPC_DESCRIPTOR_WHT = 1\n",
      "    INTERP_EPIC = 1\n",
      "    INTERP_GEO = 0\n",
      "    INTERP_RIC = 2\n",
      "    SR_CROSS = 1\n",
      "    SR_FIXED = 0\n",
      "    ST_BILINEAR = 1\n",
      "    ST_STANDART = 0\n",
      "\n",
      "FILE\n",
      "    (built-in)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.optflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__\n",
    "cv2.cuda.getCudaEnabledDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #parser = ArgumentParser()\n",
    "    #parser.add_argument(\n",
    "    #    \"--algorithm\",\n",
    "    #    choices=[\"farneback\", \"lucaskanade\", \"lucaskanade_dense\", \"rlof\"],\n",
    "    #    required=True,\n",
    "    #    help=\"Optical flow algorithm to use\",\n",
    "    #)\n",
    "    #parser.add_argument(\n",
    "    #    \"--video_path\", default=\"videos/cat.mp4\", help=\"Path to the video\",\n",
    "    #)\n",
    "\n",
    "    video_path = \"videos/people.mp4\"\n",
    "    algorithm = \"lucaskanade_dense\"\n",
    "    \n",
    "    if algorithm == \"lucaskanade\":\n",
    "        lucas_kanade_method(video_path)\n",
    "    elif algorithm == \"lucaskanade_dense\":\n",
    "        method = cv2.optflow.calcOpticalFlowSparseToDense\n",
    "        dense_optical_flow(method, video_path, to_gray=True)\n",
    "    elif algorithm == \"farneback\":\n",
    "        method = cv2.calcOpticalFlowFarneback\n",
    "        params = [0.5, 3, 15, 3, 5, 1.2, 0]  # Farneback's algorithm parameters\n",
    "        dense_optical_flow(method, video_path, params, to_gray=True)\n",
    "    elif algorithm == \"rlof\":\n",
    "        method = cv2.optflow.calcOpticalFlowDenseRLOF\n",
    "        dense_optical_flow(method, video_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--data_root DATA_ROOT]\n",
      "                             [--new_dir NEW_DIR] [--num_workers NUM_WORKERS]\n",
      "                             [--step STEP] [--bound BOUND] [--s_ S_] [--e_ E_]\n",
      "                             [--mode MODE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9017 --control=9015 --hb=9014 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"93d84e49-ec86-42a9-bad2-3832b73dce1d\" --shell=9016 --transport=\"tcp\" --iopub=9018 --f=c:\\Users\\giorg\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-37516VFNj5rDf3KIa.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "\n",
    "    # example: if the data path not setted from args,just manually set them as belows.\n",
    "    #dataset='ucf101'\n",
    "    #data_root='/S2/MI/zqj/video_classification/data'\n",
    "    #data_root=os.path.join(data_root,dataset)\n",
    "\n",
    "    args=parse_args('')\n",
    "    # data_root=os.path.join(args.data_root,args.dataset)\n",
    "    data_root= \"C:\\\\Users\\\\giorg\\\\OneDrive - Università degli Studi di Milano-Bicocca\\\\Laurea Magistrale - Data Science\\\\directory_progetti\\\\deep-learning-video-classification\\\\data\\\\hmdb51\"\n",
    "    videos_root=os.path.join(data_root,'videos')\n",
    "\n",
    "    #specify the augments\n",
    "    \n",
    "    #num_workers=args.num_workers\n",
    "    step=args.step\n",
    "    bound=args.bound\n",
    "    s_=args.s_\n",
    "    e_=args.e_\n",
    "    new_dir=args.new_dir\n",
    "    mode=args.mode\n",
    "    #get video list\n",
    "    video_list,len_videos=get_video_list()\n",
    "    video_list=video_list[s_:e_]\n",
    "\n",
    "    len_videos=min(e_-s_,13320-s_) # if we choose the ucf101\n",
    "    print('find {} videos.').format(len_videos)\n",
    "    flows_dirs=[video.split('.')[0] for video in video_list]\n",
    "    print('get videos list done! ')\n",
    "\n",
    "    pool=Pool(num_workers)\n",
    "    if mode=='run':\n",
    "        pool.map(dense_flow,zip(video_list,flows_dirs,[step]*len(video_list),[bound]*len(video_list)))\n",
    "    else: #mode=='debug\n",
    "        dense_flow((video_list[0],flows_dirs[0],step,bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_df(split):\n",
    "    df = pd.DataFrame(columns=['action', 'title', 'train/test'])\n",
    "    for action in actions:\n",
    "        with open(f\"./annotations/{action}_test_{split}.txt\") as f:\n",
    "            informations = f.readlines()       \n",
    "        for info in informations:   #per ogni video della lista dei video di una data azione\n",
    "            df.loc[len(df)] = action, info.split()[0], int(info.split()[1])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = split_to_df(\"split1\")\n",
    "split2 = split_to_df(\"split2\")\n",
    "split3 = split_to_df(\"split3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_1.avi', 'brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_2.avi', 'brush_hair\\\\atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_0.avi', 'brush_hair\\\\atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_1.avi']\n",
      "['April_09_brush_hair_u_nm_np1_ba_goo_1.avi', 'April_09_brush_hair_u_nm_np1_ba_goo_2.avi', 'atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_0.avi', 'atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_1.avi']\n"
     ]
    }
   ],
   "source": [
    "videos_root = \"C:\\\\Users\\\\giorg\\\\OneDrive - Università degli Studi di Milano-Bicocca\\\\Laurea Magistrale - Data Science\\\\directory_progetti\\\\deep-learning-video-classification\\\\data\\\\hmdb51\\\\videos\"\n",
    "cls_names_list = os.listdir(videos_root)\n",
    "video_list=[]\n",
    "video_cls_path_list=[]\n",
    "for cls_names in os.listdir(videos_root):\n",
    "    cls_path=os.path.join(videos_root,cls_names)\n",
    "    for video_ in os.listdir(cls_path):\n",
    "        video_cls_path = os.path.join(cls_names, video_)\n",
    "        video_list.append(video_)\n",
    "        video_cls_path_list.append(video_cls_path)\n",
    "#video_list.sort()\n",
    "\n",
    "print(video_cls_path_list[1:5])\n",
    "print(video_list[1:5])\n",
    "#print(cls_names_list)\n",
    "#flows_dirs=[video.split('.')[0] for video in video_list]\n",
    "#print(flows_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "import argparse\n",
    "from IPython import embed #to debug\n",
    "import scipy.misc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import skvideo\n",
    "skvideo.setFFmpegPath(\"C:\\\\ffmpeg\\\\bin\")\n",
    "import skvideo.io\n",
    "\n",
    "\n",
    "videos_root = \"C:\\\\Users\\\\giorg\\\\OneDrive - Università degli Studi di Milano-Bicocca\\\\Laurea Magistrale - Data Science\\\\directory_progetti\\\\deep-learning-video-classification\\\\data\\\\hmdb51\\\\videos\"\n",
    "skvideo.io.vread(os.path.join(videos_root,'brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_1' + '.avi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "videos_root -> dove ci sono le cartelle delle classi (hmdb/videos)\n",
    "new_dir = cartella generale dell'output (rawframes)\n",
    "data_root = cartella contenente tutto (hdbmi)\n",
    "save_dir: save_dir name (always equal to the video id) -> cartella nome del video\n",
    "param num: the save id, which belongs one of the extracted frames (nome del video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed argouments:\n",
      "Namespace(bound=20, data_root='/n/zqj/video_classification/data', dataset='hmdb51', e_=6766, mode='run', new_dir='rawframes', num_workers=4, s_=0, step=1)\n",
      "Videos root: c:\\Users\\giorg\\OneDrive - Università degli Studi di Milano-Bicocca\\Laurea Magistrale - Data Science\\directory_progetti\\deep-learning-video-classification\\data\\hmdb51\\videos\n",
      "find 6766 videos.\n",
      "get videos list done! \n",
      "Run mode activated\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\multiprocess\\pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\multiprocess\\pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"C:\\Users\\giorg\\AppData\\Local\\Temp\\ipykernel_26884\\504740452.py\", line 92, in dense_flow\nNameError: name 'os' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26884\\504740452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#modalità run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Run mode activated'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_flow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflows_dirs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflows_dirs_cls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#mode=='debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Debug mode activated'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore', lineno=38)\n",
    "#warnings.simplefilter('ignore')\n",
    "\n",
    "# BACKUP \n",
    "import os,sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# sys.modules['__main__'].__file__ = 'ipython'\n",
    "# import multiprocessing\n",
    "# from multiprocessing import Pool\n",
    "from multiprocess import Pool\n",
    "\n",
    "import argparse\n",
    "from IPython import embed #to debug\n",
    "import imageio\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import skvideo\n",
    "skvideo.setFFmpegPath(\"C:\\\\ffmpeg\\\\bin\")\n",
    "import skvideo.io\n",
    "\n",
    "import scipy.misc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def ToImg(raw_flow,bound):\n",
    "    '''\n",
    "    this function scale the input pixels to 0-255 with bi-bound\n",
    "    :param raw_flow: input raw pixel value (not in 0-255)\n",
    "    :param bound: upper and lower bound (-bound, bound)\n",
    "    :return: pixel value scale from 0 to 255\n",
    "    '''\n",
    "    flow=raw_flow\n",
    "    flow[flow>bound]=bound\n",
    "    flow[flow<-bound]=-bound\n",
    "    flow-=-bound\n",
    "    flow*=(255/float(2*bound))\n",
    "    return flow\n",
    "\n",
    "def save_flows(flows,image,save_dir,save_dir_cls,num,bound):\n",
    "    '''\n",
    "    To save the optical flow images and raw images\n",
    "    :param flows: contains flow_x and flow_y\n",
    "    :param image: raw image\n",
    "    :param save_dir: save_dir name (always equal to the video id)\n",
    "    :param save_dir_cls: path of save dire name in form of \"class//video_name\"\n",
    "    :param num: the save id, which belongs one of the extracted frames\n",
    "    :param bound: set the bi-bound to flow images\n",
    "    :return: return 0\n",
    "    '''\n",
    "    \n",
    "    #rescale to 0~255 with the bound setting\n",
    "    flow_x=ToImg(flows[...,0],bound)\n",
    "    flow_y=ToImg(flows[...,1],bound)\n",
    "    if not os.path.exists(os.path.join(data_root,new_dir,save_dir_cls)): #new_dir è il nome della cartella di salvataggio dell'output all'interno di 'data_root', di default è videos\n",
    "        os.makedirs(os.path.join(data_root,new_dir,save_dir_cls))\n",
    "\n",
    "    #save the image\n",
    "    save_img=os.path.join(data_root,new_dir,save_dir_cls,'img_{:05d}.jpg'.format(num)) # prefisso immagini: img_\n",
    "    # scipy.misc.imsave(save_img,image)\n",
    "    # cv2.imwrite(save_img, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))  \n",
    "    imageio.imwrite(save_img,image)  \n",
    "\n",
    "    #save the flows\n",
    "    save_x=os.path.join(data_root,new_dir,save_dir_cls,'flow_x_{:05d}.jpg'.format(num)) # prefisso optical flow x: flow_x_\n",
    "    save_y=os.path.join(data_root,new_dir,save_dir_cls,'flow_y_{:05d}.jpg'.format(num)) # prefisso optical flow y: flow_y_\n",
    "    flow_x_img=Image.fromarray(flow_x)\n",
    "    flow_y_img=Image.fromarray(flow_y)\n",
    "    #scipy.misc.imsave(save_x,flow_x_img)\n",
    "    imageio.imwrite(save_x,flow_x_img)\n",
    "    #scipy.misc.imsave(save_y,flow_y_img)\n",
    "    imageio.imwrite(save_y,flow_y_img)\n",
    "    return 0\n",
    "\n",
    "def dense_flow(augs):\n",
    "    '''\n",
    "    To extract dense_flow images\n",
    "    :param augs:the detailed augments:\n",
    "        video_name: the video name which is like: 'v_xxxxxxx',if different ,please have a modify.\n",
    "        save_dir: the destination path's final direction name. video_name (without .ext)\n",
    "        save_dir_cls: added by me. class_name//video_name (without .ext)\n",
    "        step: num of frames between each two extracted frames\n",
    "        bound: bi-bound parameter\n",
    "    :return: no returns\n",
    "    '''\n",
    "    print('Dense Flow function activated')\n",
    "    video_name,save_dir,save_dir_cls,step,bound=augs #flow_dir viene passato come save_dir (essenzialmente è il nome del video senza avi)\n",
    "    # video_path=os.path.join(videos_root,video_name.split('_')[1],video_name) # CAMBIATO TOLTO LO SPLIT\n",
    "    video_path=os.path.join(videos_root,save_dir_cls + '.avi') \n",
    "    # provide two video-read methods: cv2.VideoCapture() and skvideo.io.vread(), both of which need ffmpeg support\n",
    "\n",
    "    # videocapture=cv2.VideoCapture(video_path)\n",
    "    # if not videocapture.isOpened():\n",
    "    #     print 'Could not initialize capturing! ', video_name\n",
    "    #     exit()\n",
    "    \n",
    "    print(\"Parsed argouments:\")\n",
    "    print(args)\n",
    "    print(f'Videos root: {videos_root}')\n",
    "    \n",
    "    try:\n",
    "        videocapture=skvideo.io.vread(video_path)\n",
    "    except:\n",
    "        print('{} read error! ').format(video_name)\n",
    "        return 0\n",
    "    print(video_name)\n",
    "    # if extract nothing, exit!\n",
    "    if videocapture.sum()==0:\n",
    "        print('Could not initialize capturing'),video_name\n",
    "        exit()\n",
    "    else: print(f'Inizialize capturing of {video_path}')\n",
    "    len_frame=len(videocapture)\n",
    "    frame_num=0\n",
    "    image,prev_image,gray,prev_gray=None,None,None,None\n",
    "    num0=0\n",
    "    while True:\n",
    "        #frame=videocapture.read()\n",
    "        if num0>=len_frame:\n",
    "            break\n",
    "        frame=videocapture[num0]\n",
    "        num0+=1\n",
    "        if frame_num==0:\n",
    "            image=np.zeros_like(frame)\n",
    "            gray=np.zeros_like(frame)\n",
    "            prev_gray=np.zeros_like(frame)\n",
    "            prev_image=frame\n",
    "            prev_gray=cv2.cvtColor(prev_image,cv2.COLOR_RGB2GRAY)\n",
    "            frame_num+=1\n",
    "            # to pass the out of stepped frames\n",
    "            step_t=step\n",
    "            while step_t>1:\n",
    "                #frame=videocapture.read()\n",
    "                num0+=1\n",
    "                step_t-=1\n",
    "            continue\n",
    "\n",
    "        image=frame\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "        frame_0=prev_gray\n",
    "        frame_1=gray\n",
    "        ##default choose the tvl1 algorithm\n",
    "        dtvl1=cv2.optflow.createOptFlow_DualTVL1()\n",
    "        flowDTVL1=dtvl1.calc(frame_0,frame_1,None)\n",
    "        print(f'Salvataggio frame {frame_num}')\n",
    "        save_flows(flowDTVL1,image,save_dir,save_dir_cls,frame_num,bound) #this is to save flows and img.\n",
    "        prev_gray=gray\n",
    "        prev_image=image\n",
    "        frame_num+=1\n",
    "        # to pass the out of stepped frames\n",
    "        step_t=step\n",
    "        while step_t>1:\n",
    "            #frame=videocapture.read()\n",
    "            num0+=1\n",
    "            step_t-=1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Funzione che trova 1) La lista di tutti i nomi dei video 2) La lunghezza di tale lista (quanti video ci sono)\n",
    "'''\n",
    "def get_video_list():\n",
    "    cls_names_list = os.listdir(videos_root)\n",
    "    video_list=[]\n",
    "    video_cls_path_list=[]\n",
    "    for cls_names in os.listdir(videos_root):\n",
    "        cls_path=os.path.join(videos_root,cls_names)\n",
    "        for video_ in os.listdir(cls_path):\n",
    "            video_cls_path = os.path.join(cls_names, video_)\n",
    "            video_list.append(video_)\n",
    "            video_cls_path_list.append(video_cls_path)\n",
    "    #video_list.sort()\n",
    "    return video_list,len(video_list),video_cls_path_list\n",
    "\n",
    "\n",
    "#  step         | 0    | right - left (0 for img, non-0 for flow) / step is 1, ie flow of adjacent frames\n",
    "#  bound        | 32   | maximum of optical flow\n",
    "# START_IDX (s_): The start index of extracted frames.\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"densely extract the video frames and optical flows\")\n",
    "    parser.add_argument('--dataset',default='hmdb51',type=str,help='set the dataset name, to find the data path')\n",
    "    parser.add_argument('--data_root',default='/n/zqj/video_classification/data',type=str)\n",
    "    parser.add_argument('--new_dir',default='rawframes',type=str) # cambiato il default prima era 'flows'\n",
    "    parser.add_argument('--num_workers',default=4,type=int,help='num of workers to act multi-process')\n",
    "    parser.add_argument('--step',default=1,type=int,help='gap frames')\n",
    "    parser.add_argument('--bound',default=15,type=int,help='set the maximum of optical flow') # cambiato il default prima era '15'\n",
    "    # nel codice otterremo la lista di tutti i video e poi una sottolista da essi\n",
    "    parser.add_argument('--s_',default=0,type=int,help='start id') # indica l'indice nella lista da cui parte la sottolista\n",
    "    parser.add_argument('--e_',default=6766,type=int,help='end id') # indica l'indice in cui finisce la sottolista (default = 6766 ovvero il numero di video)\n",
    "    parser.add_argument('--mode',default='run',type=str,help='set \\'run\\' if debug done, otherwise, set debug')\n",
    "    args = parser.parse_args('') # mettendo come argomento le '' vengono passati gli argomenti base\n",
    "    return args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # example: if the data path not setted from args,just manually set them as belows.\n",
    "    #dataset='ucf101'\n",
    "    #data_root='/S2/MI/zqj/video_classification/data'\n",
    "    #data_root=os.path.join(data_root,dataset)\n",
    "\n",
    "    args=parse_args()\n",
    "    \n",
    "    cwd = Path.cwd()\n",
    "    dataset= \"hmdb51\"\n",
    "    data_root = os.path.join(Path.cwd(),dataset)\n",
    "    videos_root= os.path.join(data_root,'videos')\n",
    "\n",
    "    #specify the augments\n",
    "    num_workers=args.num_workers\n",
    "    step=args.step\n",
    "    bound=args.bound\n",
    "    s_=args.s_\n",
    "    e_=args.e_\n",
    "    #new_dir=args.new_dir\n",
    "    new_dir = \"rawframes\"\n",
    "    #mode= args.mode\n",
    "    mode = \"run\"\n",
    "    \n",
    "    print(\"Parsed argouments:\")\n",
    "    print(args)\n",
    "    print(f'Videos root: {videos_root}')\n",
    "    \n",
    "    #get video list\n",
    "    video_list,len_videos,video_cls_path_list=get_video_list()\n",
    "    video_list=video_list[s_:e_] #seleziona la sottolista\n",
    "    video_cls_path_list=video_cls_path_list[s_:e_] #seleziona sottolista di save/flow_dir\n",
    "\n",
    "    len_videos=min(e_-s_,6766-s_) # numero totale di video selezionati\n",
    "    print(f'find {len_videos} videos.')\n",
    "    #print('find {} videos.').format(len_videos)\n",
    "    flows_dirs=[video.split('.')[0] for video in video_list] # genera una lista dei video senza il '.avi' (nome della cartella)\n",
    "    flows_dirs_cls =[video.split('.')[0] for video in video_cls_path_list] # genera una lista dei video senza il '.avi' per i path\n",
    "    \n",
    "    print('get videos list done! ')\n",
    "\n",
    "    # Parallelizzazione\n",
    "    pool = Pool(num_workers) # setta il numero di processi paralleli\n",
    "    #print([len(video_list),len(flows_dirs),len(flows_dirs_cls),len([step]*len(video_list)),len([bound]*len(video_list))])\n",
    "    \n",
    "    if mode=='run': #modalità run\n",
    "        print('Run mode activated')\n",
    "        pool.map(dense_flow,zip(video_list,flows_dirs,flows_dirs_cls,[step]*len(video_list),[bound]*len(video_list)))\n",
    "    else: #mode=='debug\n",
    "        print('Debug mode activated')\n",
    "        dense_flow((video_list[0],flows_dirs[0],flows_dirs_cls[0],step,bound))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    clear_output(wait=True)\n",
    "    print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\OneDrive - Università degli Studi di Milano-Bicocca\\Laurea Magistrale - Data Science\\directory_progetti\\deep-learning-video-classification\\data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\giorg\\\\OneDrive - Università degli Studi di Milano-Bicocca\\\\Laurea Magistrale - Data Science\\\\directory_progetti\\\\deep-learning-video-classification\\\\data\\\\hmdb51.avi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23916\\71342998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"hmdb51\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.avi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\giorg\\\\OneDrive - Università degli Studi di Milano-Bicocca\\\\Laurea Magistrale - Data Science\\\\directory_progetti\\\\deep-learning-video-classification\\\\data\\\\hmdb51.avi'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "print(cwd)\n",
    "print(os.listdir(os.path.join(cwd,\"hmdb51\"+'.avi')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed argouments:\n",
      "Namespace(bound=20, data_root='/n/zqj/video_classification/data', dataset='hmdb51', e_=6766, mode='run', new_dir='rawframes', num_workers=4, s_=0, step=1)\n",
      "Videos root: c:\\Users\\giorg\\OneDrive - Università degli Studi di Milano-Bicocca\\Laurea Magistrale - Data Science\\directory_progetti\\deep-learning-video-classification\\data\\hmdb51\\videos\n",
      "find 6766 videos.\n",
      "get videos list done! \n",
      "Run mode activated\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore', lineno=38)\n",
    "#warnings.simplefilter('ignore')\n",
    "\n",
    "# BACKUP \n",
    "import os,sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "sys.modules['__main__'].__file__ = 'ipython'\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import argparse\n",
    "from IPython import embed #to debug\n",
    "import imageio\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import skvideo\n",
    "skvideo.setFFmpegPath(\"C:\\\\ffmpeg\\\\bin\")\n",
    "import skvideo.io\n",
    "\n",
    "import scipy.misc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def ToImg(raw_flow,bound):\n",
    "    '''\n",
    "    this function scale the input pixels to 0-255 with bi-bound\n",
    "    :param raw_flow: input raw pixel value (not in 0-255)\n",
    "    :param bound: upper and lower bound (-bound, bound)\n",
    "    :return: pixel value scale from 0 to 255\n",
    "    '''\n",
    "    flow=raw_flow\n",
    "    flow[flow>bound]=bound\n",
    "    flow[flow<-bound]=-bound\n",
    "    flow-=-bound\n",
    "    flow*=(255/float(2*bound))\n",
    "    return flow\n",
    "\n",
    "def save_flows(flows,image,save_dir,save_dir_cls,num,bound):\n",
    "    '''\n",
    "    To save the optical flow images and raw images\n",
    "    :param flows: contains flow_x and flow_y\n",
    "    :param image: raw image\n",
    "    :param save_dir: save_dir name (always equal to the video id)\n",
    "    :param save_dir_cls: path of save dire name in form of \"class//video_name\"\n",
    "    :param num: the save id, which belongs one of the extracted frames\n",
    "    :param bound: set the bi-bound to flow images\n",
    "    :return: return 0\n",
    "    '''\n",
    "    \n",
    "    #rescale to 0~255 with the bound setting\n",
    "    flow_x=ToImg(flows[...,0],bound)\n",
    "    flow_y=ToImg(flows[...,1],bound)\n",
    "    if not os.path.exists(os.path.join(data_root,new_dir,save_dir_cls)): #new_dir è il nome della cartella di salvataggio dell'output all'interno di 'data_root', di default è videos\n",
    "        os.makedirs(os.path.join(data_root,new_dir,save_dir_cls))\n",
    "\n",
    "    #save the image\n",
    "    save_img=os.path.join(data_root,new_dir,save_dir_cls,'img_{:05d}.jpg'.format(num)) # prefisso immagini: img_\n",
    "    # scipy.misc.imsave(save_img,image)\n",
    "    # cv2.imwrite(save_img, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))  \n",
    "    imageio.imwrite(save_img,image)  \n",
    "\n",
    "    #save the flows\n",
    "    save_x=os.path.join(data_root,new_dir,save_dir_cls,'flow_x_{:05d}.jpg'.format(num)) # prefisso optical flow x: flow_x_\n",
    "    save_y=os.path.join(data_root,new_dir,save_dir_cls,'flow_y_{:05d}.jpg'.format(num)) # prefisso optical flow y: flow_y_\n",
    "    flow_x_img=Image.fromarray(flow_x)\n",
    "    flow_y_img=Image.fromarray(flow_y)\n",
    "    #scipy.misc.imsave(save_x,flow_x_img)\n",
    "    imageio.imwrite(save_x,flow_x_img)\n",
    "    #scipy.misc.imsave(save_y,flow_y_img)\n",
    "    imageio.imwrite(save_y,flow_y_img)\n",
    "    return 0\n",
    "\n",
    "def dense_flow(augs):\n",
    "    '''\n",
    "    To extract dense_flow images\n",
    "    :param augs:the detailed augments:\n",
    "        video_name: the video name which is like: 'v_xxxxxxx',if different ,please have a modify.\n",
    "        save_dir: the destination path's final direction name. video_name (without .ext)\n",
    "        save_dir_cls: added by me. class_name//video_name (without .ext)\n",
    "        step: num of frames between each two extracted frames\n",
    "        bound: bi-bound parameter\n",
    "    :return: no returns\n",
    "    '''\n",
    "    print('Dense Flow function activated')\n",
    "    video_name,save_dir,save_dir_cls,step,bound=augs #flow_dir viene passato come save_dir (essenzialmente è il nome del video senza avi)\n",
    "    # video_path=os.path.join(videos_root,video_name.split('_')[1],video_name) # CAMBIATO TOLTO LO SPLIT\n",
    "    video_path=os.path.join(videos_root,save_dir_cls + '.avi') \n",
    "    # provide two video-read methods: cv2.VideoCapture() and skvideo.io.vread(), both of which need ffmpeg support\n",
    "\n",
    "    # videocapture=cv2.VideoCapture(video_path)\n",
    "    # if not videocapture.isOpened():\n",
    "    #     print 'Could not initialize capturing! ', video_name\n",
    "    #     exit()\n",
    "    \n",
    "    print(\"Parsed argouments:\")\n",
    "    print(args)\n",
    "    print(f'Videos root: {videos_root}')\n",
    "    \n",
    "    try:\n",
    "        videocapture=skvideo.io.vread(video_path)\n",
    "    except:\n",
    "        print('{} read error! ').format(video_name)\n",
    "        return 0\n",
    "    print(video_name)\n",
    "    # if extract nothing, exit!\n",
    "    if videocapture.sum()==0:\n",
    "        print('Could not initialize capturing'),video_name\n",
    "        exit()\n",
    "    else: print(f'Inizialize capturing of {video_path}')\n",
    "    len_frame=len(videocapture)\n",
    "    frame_num=0\n",
    "    image,prev_image,gray,prev_gray=None,None,None,None\n",
    "    num0=0\n",
    "    while True:\n",
    "        #frame=videocapture.read()\n",
    "        if num0>=len_frame:\n",
    "            break\n",
    "        frame=videocapture[num0]\n",
    "        num0+=1\n",
    "        if frame_num==0:\n",
    "            image=np.zeros_like(frame)\n",
    "            gray=np.zeros_like(frame)\n",
    "            prev_gray=np.zeros_like(frame)\n",
    "            prev_image=frame\n",
    "            prev_gray=cv2.cvtColor(prev_image,cv2.COLOR_RGB2GRAY)\n",
    "            frame_num+=1\n",
    "            # to pass the out of stepped frames\n",
    "            step_t=step\n",
    "            while step_t>1:\n",
    "                #frame=videocapture.read()\n",
    "                num0+=1\n",
    "                step_t-=1\n",
    "            continue\n",
    "\n",
    "        image=frame\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "        frame_0=prev_gray\n",
    "        frame_1=gray\n",
    "        ##default choose the tvl1 algorithm\n",
    "        dtvl1=cv2.optflow.createOptFlow_DualTVL1()\n",
    "        flowDTVL1=dtvl1.calc(frame_0,frame_1,None)\n",
    "        print(f'Salvataggio frame {frame_num}')\n",
    "        save_flows(flowDTVL1,image,save_dir,save_dir_cls,frame_num,bound) #this is to save flows and img.\n",
    "        prev_gray=gray\n",
    "        prev_image=image\n",
    "        frame_num+=1\n",
    "        # to pass the out of stepped frames\n",
    "        step_t=step\n",
    "        while step_t>1:\n",
    "            #frame=videocapture.read()\n",
    "            num0+=1\n",
    "            step_t-=1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Funzione che trova 1) La lista di tutti i nomi dei video 2) La lunghezza di tale lista (quanti video ci sono)\n",
    "'''\n",
    "def get_video_list():\n",
    "    cls_names_list = os.listdir(videos_root)\n",
    "    video_list=[]\n",
    "    video_cls_path_list=[]\n",
    "    for cls_names in os.listdir(videos_root):\n",
    "        cls_path=os.path.join(videos_root,cls_names)\n",
    "        for video_ in os.listdir(cls_path):\n",
    "            video_cls_path = os.path.join(cls_names, video_)\n",
    "            video_list.append(video_)\n",
    "            video_cls_path_list.append(video_cls_path)\n",
    "    #video_list.sort()\n",
    "    return video_list,len(video_list),video_cls_path_list\n",
    "\n",
    "\n",
    "#  step         | 0    | right - left (0 for img, non-0 for flow) / step is 1, ie flow of adjacent frames\n",
    "#  bound        | 32   | maximum of optical flow\n",
    "# START_IDX (s_): The start index of extracted frames.\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"densely extract the video frames and optical flows\")\n",
    "    parser.add_argument('--dataset',default='hmdb51',type=str,help='set the dataset name, to find the data path')\n",
    "    parser.add_argument('--data_root',default='/n/zqj/video_classification/data',type=str)\n",
    "    parser.add_argument('--new_dir',default='rawframes',type=str) # cambiato il default prima era 'flows'\n",
    "    parser.add_argument('--num_workers',default=4,type=int,help='num of workers to act multi-process')\n",
    "    parser.add_argument('--step',default=1,type=int,help='gap frames')\n",
    "    parser.add_argument('--bound',default=20,type=int,help='set the maximum of optical flow') # cambiato il default prima era '15'\n",
    "    # nel codice otterremo la lista di tutti i video e poi una sottolista da essi\n",
    "    parser.add_argument('--s_',default=0,type=int,help='start id') # indica l'indice nella lista da cui parte la sottolista\n",
    "    parser.add_argument('--e_',default=6766,type=int,help='end id') # indica l'indice in cui finisce la sottolista (default = 6766 ovvero il numero di video)\n",
    "    parser.add_argument('--mode',default='run',type=str,help='set \\'run\\' if debug done, otherwise, set debug')\n",
    "    args = parser.parse_args('') # mettendo come argomento le '' vengono passati gli argomenti base\n",
    "    return args\n",
    "\n",
    "# example: if the data path not setted from args,just manually set them as belows.\n",
    "#dataset='ucf101'\n",
    "#data_root='/S2/MI/zqj/video_classification/data'\n",
    "#data_root=os.path.join(data_root,dataset)\n",
    "\n",
    "args=parse_args()\n",
    "\n",
    "cwd = Path.cwd()\n",
    "dataset= \"hmdb51\"\n",
    "data_root = os.path.join(Path.cwd(),dataset)\n",
    "videos_root= os.path.join(data_root,'videos')\n",
    "\n",
    "#specify the augments\n",
    "num_workers=args.num_workers\n",
    "step=args.step\n",
    "bound=args.bound\n",
    "s_=args.s_\n",
    "e_=args.e_\n",
    "#new_dir=args.new_dir\n",
    "new_dir = \"rawframes\"\n",
    "#mode= args.mode\n",
    "mode = \"run\"\n",
    "\n",
    "print(\"Parsed argouments:\")\n",
    "print(args)\n",
    "print(f'Videos root: {videos_root}')\n",
    "\n",
    "#get video list\n",
    "video_list,len_videos,video_cls_path_list=get_video_list()\n",
    "video_list=video_list[s_:e_] #seleziona la sottolista\n",
    "video_cls_path_list=video_cls_path_list[s_:e_] #seleziona sottolista di save/flow_dir\n",
    "\n",
    "len_videos=min(e_-s_,6766-s_) # numero totale di video selezionati\n",
    "print(f'find {len_videos} videos.')\n",
    "#print('find {} videos.').format(len_videos)\n",
    "flows_dirs=[video.split('.')[0] for video in video_list] # genera una lista dei video senza il '.avi' (nome della cartella)\n",
    "flows_dirs_cls =[video.split('.')[0] for video in video_cls_path_list] # genera una lista dei video senza il '.avi' per i path\n",
    "\n",
    "print('get videos list done! ')\n",
    "\n",
    "# Parallelizzazione\n",
    "pool = Pool(num_workers) # setta il numero di processi paralleli\n",
    "#print([len(video_list),len(flows_dirs),len(flows_dirs_cls),len([step]*len(video_list)),len([bound]*len(video_list))])\n",
    "\n",
    "if mode=='run': #modalità run\n",
    "    print('Run mode activated')\n",
    "    pool.map(dense_flow,zip(video_list,flows_dirs,flows_dirs_cls,[step]*len(video_list),[bound]*len(video_list)))\n",
    "else: #mode=='debug\n",
    "    print('Debug mode activated')\n",
    "    dense_flow((video_list[0],flows_dirs[0],flows_dirs_cls[0],step,bound))\n",
    "pool.close()\n",
    "pool.join()\n",
    "clear_output(wait=True)\n",
    "print('FINISHED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deepL37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f73b1806a9ec290aebb7f16dd5394fd2c9a0b8bec90a8791426b7ef5a3d9604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
