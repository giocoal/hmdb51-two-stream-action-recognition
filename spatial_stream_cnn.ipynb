{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff3c55f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe10128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.core.display import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from keras.layers.core import Dense,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4a195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Desktop\\deep-learning-video-classification\n"
     ]
    }
   ],
   "source": [
    "#path dove si trova il dataframe hmdb51\n",
    "path = Path.cwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c48d39",
   "metadata": {},
   "source": [
    "Testing if cuda is on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bff241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802b407",
   "metadata": {},
   "source": [
    "# Spatial Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/hmdb51'\n",
    "path_rowframes = './data/hmdb51/rawframes/'\n",
    "path_annotations = './data/hmdb51/annotations/'\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 128\n",
    "num_classes = 51\n",
    "\n",
    "num_frames_desired = 17     #number of frames per clip\n",
    "type_frame = 'img'          #img / flow_x / flow_y\n",
    "epochs = 200\n",
    "# partition = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  # layers.Resizing(img_height, img_height),\n",
    "  tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e047eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.20, fill_mode = \"nearest\"),\n",
    "    # layers.RandomZoom(0.2),\n",
    "    layers.RandomCrop(img_height,img_width),\n",
    "    # layers.RandomHeight(0.2),\n",
    "    # layers.RandomWidth(0.2),\n",
    "    # layers.Resizing(224,224)\n",
    "    layers.Resizing(img_height, img_width)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb40a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(list, num_frames_desired):\n",
    "    step = len(list) // (num_frames_desired)\n",
    "    #selected_frames = list(range(0, len(list), step))[:num_frames_desired]\n",
    "    sampled_list = list[0:len(list):step][:num_frames_desired]\n",
    "    return(sampled_list)\n",
    "\n",
    "def parse_image(filename):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # image = tf.image.random_flip_left_right(image)\n",
    "    # image = tf.image.resize(image, [img_height, img_width])\n",
    "    return image\n",
    "\n",
    "\"\"\"def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat() # repeat determines how many times the samples can be repeated in the given dataset. (def = 0)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds\"\"\"\n",
    "    \n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.cache()\n",
    "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000)\n",
    "  # Batch all datasets.\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.repeat() # repeat determines how many times the samples can be repeated in the given dataset. (def = 0)\n",
    "  # Use data augmentation only on the training set.\n",
    "  if augment:\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def find_paths(partition, type_frame, num_frames_desired):\n",
    "    if partition == 'train':\n",
    "        video_list = pd.read_csv(f'{path_annotations}/hmdb51_train_split_1_rawframes.txt', sep=\" \", header=None) #train\n",
    "        video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "    elif partition == 'val':\n",
    "        video_list = pd.read_csv(f'{path_annotations}/hmdb51_val_split_1_rawframes.txt', sep=\" \", header=None) #test\n",
    "        video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "    else:\n",
    "        raise Exception(\"invalid partition\")\n",
    "\n",
    "    #temp_path = video_list.loc[0]['path'] #da togliere!!!\n",
    "\n",
    "    paths = []\n",
    "    classes = []\n",
    "    for index, row in video_list.iterrows(): #da togliere [:1]\n",
    "        temp_path = row['path']                    #da rimuovere il commentato\n",
    "        frame_list = os.listdir(os.path.join(f'./{temp_path}'))\n",
    "\n",
    "        frame_list_type = [i for i in frame_list if i.startswith(f'{type_frame}')]\n",
    "\n",
    "        filename = sampling(frame_list_type, num_frames_desired)\n",
    "\n",
    "        paths.extend([os.path.join('.\\\\', temp_path, file) for file in filename])\n",
    "        temp = [row['class']] * num_frames_desired\n",
    "        classes.extend(temp)\n",
    "\n",
    "    #return(list(zip(paths, classes)))\n",
    "    return(list(zip(paths, classes)), video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6384ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- create train set\n",
    "filenames, video_list = find_paths(partition='train', type_frame=type_frame, num_frames_desired=num_frames_desired)\n",
    "video_list[['cose', 'class', 'vid']] = video_list[\"path\"].str.rsplit(\"\\\\\", n = 2, expand = True)\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "zipped = [list(t) for t in zip(*filenames)]\n",
    "\n",
    "filenames = zipped[0]\n",
    "labels = zipped[1]\n",
    "\n",
    "filenames_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "images_ds = filenames_ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "# train_ds = configure_for_performance(ds)\n",
    "train_ds = prepare(ds, shuffle=True, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71781ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60690\n"
     ]
    }
   ],
   "source": [
    "frame_number_train = len(filenames)\n",
    "print(frame_number_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a40fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- create val test\n",
    "filenames, video_list = find_paths(partition='val', type_frame=type_frame, num_frames_desired=num_frames_desired)\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "zipped = [list(t) for t in zip(*filenames)]\n",
    "\n",
    "filenames = zipped[0]\n",
    "labels = zipped[1]\n",
    "\n",
    "filenames_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "images_ds = filenames_ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "# val_ds = configure_for_performance(ds)\n",
    "val_ds = prepare(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bdd738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n"
     ]
    }
   ],
   "source": [
    "frame_number_val = len(filenames)\n",
    "print(frame_number_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf13314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "step_per_epoch_train = frame_number_train // batch_size\n",
    "step_per_epoch_val = frame_number_val // batch_size\n",
    "print(step_per_epoch_train)\n",
    "print(step_per_epoch_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c56de",
   "metadata": {},
   "source": [
    "# Spatial Stream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "version = \"v4\"\n",
    "arc = \"a3\"\n",
    "checkpoint_filepath = f'./Models/spatial_stream/spatial_model_{version}_{arc}_val_acc_best.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3abfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def prep_fn(img):\n",
    "    img = img / 255.0\n",
    "    img = (img - 0.5) * 2\n",
    "    return img\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    if width >= dy:\n",
    "        top = int((height - dy) * random.random())\n",
    "        left = int((width - dx) * random.random())\n",
    "        right = left + width\n",
    "        bottom = top + height\n",
    "    else: \n",
    "        top = int((height - dy) * random.random())\n",
    "        bottom = top + height\n",
    "        left = 0\n",
    "        right = width\n",
    "    img = img[top : bottom, left : right] \n",
    "    return img\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    preprocessing_function=prep_fn,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20, fill_mode='nearest'\n",
    "    \n",
    ") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54af4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    # layers.RandomZoom(0.2),\n",
    "    layers.RandomCrop(224,224)\n",
    "    # layers.RandomHeight(0.2),\n",
    "    # layers.RandomWidth(0.2),\n",
    "    # layers.Resizing(224,224)\n",
    "])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a260b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 51\n",
    "\n",
    "# model_spat = keras.models.Sequential(tf.keras.layers.Rescaling(1./127.5, offset=-1))\n",
    "model_spat = keras.models.Sequential()\n",
    "\n",
    "#data_augmentation\n",
    "\n",
    "model_spat.add(keras.layers.Conv2D(96, (7,7), strides = 2, input_shape=(224, 224, 3), activation = \"relu\"))\n",
    "model_spat.add(keras.layers.BatchNormalization())\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(256, (5,5), strides = 2, activation='relu'))\n",
    "model_spat.add(keras.layers.BatchNormalization())\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "          \n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_spat.add(keras.layers.Flatten())\n",
    "\n",
    "model_spat.add(keras.layers.Dense(2048, activation='relu'))\n",
    "model_spat.add(keras.layers.Dropout(0.5))      #valore dropout 0.5 oppure 0.9? paper li usa entrambi \n",
    "\n",
    "model_spat.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model_spat.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model_spat.add(keras.layers.Dense(num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15246a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 109, 109, 96)      14208     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 109, 109, 96)     384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 57, 57, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " zero_padding2d_2 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " zero_padding2d_3 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 51)                104499    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,789,747\n",
      "Trainable params: 117,789,043\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_spat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36e69ae4",
   "metadata": {
    "id": "950cdb45"
   },
   "outputs": [],
   "source": [
    "#model_spat.compile(loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy','sparse_categorical_accuracy'], \n",
    "#              optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "learning_rate = 1e-2 # -2\n",
    "momentum = 0.9\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "model_spat.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                   metrics=['sparse_categorical_accuracy','sparse_top_k_categorical_accuracy'], \n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9a1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "step_per_epoch_train = frame_number_train // batch_size\n",
    "step_per_epoch_val = frame_number_val // batch_size\n",
    "print(step_per_epoch_train)\n",
    "print(step_per_epoch_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fc456a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34d7d98d",
    "outputId": "e39004e4-5c77-4d59-b9b7-054d2a29254f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "557/557 [==============================] - 252s 448ms/step - loss: 3.7728 - accuracy: 0.0414 - sparse_categorical_accuracy: 0.0414 - val_loss: 4.0078 - val_accuracy: 0.0431 - val_sparse_categorical_accuracy: 0.0431\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 193s 345ms/step - loss: 3.7258 - accuracy: 0.0452 - sparse_categorical_accuracy: 0.0452 - val_loss: 3.8919 - val_accuracy: 0.0352 - val_sparse_categorical_accuracy: 0.0352\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 130s 233ms/step - loss: 3.6958 - accuracy: 0.0516 - sparse_categorical_accuracy: 0.0516 - val_loss: 3.8123 - val_accuracy: 0.0275 - val_sparse_categorical_accuracy: 0.0275\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 87s 156ms/step - loss: 3.6447 - accuracy: 0.0559 - sparse_categorical_accuracy: 0.0559 - val_loss: 3.8038 - val_accuracy: 0.0581 - val_sparse_categorical_accuracy: 0.0581\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 87s 157ms/step - loss: 3.6303 - accuracy: 0.0654 - sparse_categorical_accuracy: 0.0654 - val_loss: 3.7834 - val_accuracy: 0.0588 - val_sparse_categorical_accuracy: 0.0588\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 3.5559 - accuracy: 0.0778 - sparse_categorical_accuracy: 0.0778 - val_loss: 3.8742 - val_accuracy: 0.0416 - val_sparse_categorical_accuracy: 0.0416\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 3.4452 - accuracy: 0.1039 - sparse_categorical_accuracy: 0.1039 - val_loss: 4.0279 - val_accuracy: 0.0544 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 84s 150ms/step - loss: 3.2219 - accuracy: 0.1545 - sparse_categorical_accuracy: 0.1545 - val_loss: 4.1645 - val_accuracy: 0.0584 - val_sparse_categorical_accuracy: 0.0584\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 2.9568 - accuracy: 0.2247 - sparse_categorical_accuracy: 0.2247 - val_loss: 4.4137 - val_accuracy: 0.0565 - val_sparse_categorical_accuracy: 0.0565\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 2.5505 - accuracy: 0.3317 - sparse_categorical_accuracy: 0.3317 - val_loss: 4.8062 - val_accuracy: 0.0532 - val_sparse_categorical_accuracy: 0.0532\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 88s 159ms/step - loss: 2.1066 - accuracy: 0.4502 - sparse_categorical_accuracy: 0.4502 - val_loss: 5.1217 - val_accuracy: 0.0756 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 1.6968 - accuracy: 0.5598 - sparse_categorical_accuracy: 0.5598 - val_loss: 5.0898 - val_accuracy: 0.0614 - val_sparse_categorical_accuracy: 0.0614\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 1.3811 - accuracy: 0.6366 - sparse_categorical_accuracy: 0.6366 - val_loss: 5.5927 - val_accuracy: 0.0628 - val_sparse_categorical_accuracy: 0.0628\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 1.1420 - accuracy: 0.6981 - sparse_categorical_accuracy: 0.6981 - val_loss: 5.7259 - val_accuracy: 0.0603 - val_sparse_categorical_accuracy: 0.0603\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.9624 - accuracy: 0.7450 - sparse_categorical_accuracy: 0.7450 - val_loss: 6.3249 - val_accuracy: 0.0580 - val_sparse_categorical_accuracy: 0.0580\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.8515 - accuracy: 0.7737 - sparse_categorical_accuracy: 0.7737 - val_loss: 6.1954 - val_accuracy: 0.0660 - val_sparse_categorical_accuracy: 0.0660\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.7366 - accuracy: 0.8025 - sparse_categorical_accuracy: 0.8025 - val_loss: 6.7322 - val_accuracy: 0.0673 - val_sparse_categorical_accuracy: 0.0673\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.6802 - accuracy: 0.8196 - sparse_categorical_accuracy: 0.8196 - val_loss: 6.7058 - val_accuracy: 0.0648 - val_sparse_categorical_accuracy: 0.0648\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.6278 - accuracy: 0.8361 - sparse_categorical_accuracy: 0.8361 - val_loss: 7.0455 - val_accuracy: 0.0609 - val_sparse_categorical_accuracy: 0.0609\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.5862 - accuracy: 0.8471 - sparse_categorical_accuracy: 0.8471 - val_loss: 7.4905 - val_accuracy: 0.0615 - val_sparse_categorical_accuracy: 0.0615\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.5543 - accuracy: 0.8537 - sparse_categorical_accuracy: 0.8537 - val_loss: 6.7894 - val_accuracy: 0.0690 - val_sparse_categorical_accuracy: 0.0690\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 89s 160ms/step - loss: 0.5050 - accuracy: 0.8663 - sparse_categorical_accuracy: 0.8663 - val_loss: 6.7712 - val_accuracy: 0.0760 - val_sparse_categorical_accuracy: 0.0760\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.4816 - accuracy: 0.8754 - sparse_categorical_accuracy: 0.8754 - val_loss: 7.4802 - val_accuracy: 0.0632 - val_sparse_categorical_accuracy: 0.0632\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.4161 - accuracy: 0.8917 - sparse_categorical_accuracy: 0.8917 - val_loss: 6.9206 - val_accuracy: 0.0671 - val_sparse_categorical_accuracy: 0.0671\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.4160 - accuracy: 0.8943 - sparse_categorical_accuracy: 0.8943 - val_loss: 7.3107 - val_accuracy: 0.0592 - val_sparse_categorical_accuracy: 0.0592\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 88s 159ms/step - loss: 0.4038 - accuracy: 0.8963 - sparse_categorical_accuracy: 0.8963 - val_loss: 7.7938 - val_accuracy: 0.0833 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.3812 - accuracy: 0.9014 - sparse_categorical_accuracy: 0.9014 - val_loss: 7.8164 - val_accuracy: 0.0780 - val_sparse_categorical_accuracy: 0.0780\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.4089 - accuracy: 0.8998 - sparse_categorical_accuracy: 0.8998 - val_loss: 7.4891 - val_accuracy: 0.0753 - val_sparse_categorical_accuracy: 0.0753\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.3653 - accuracy: 0.9074 - sparse_categorical_accuracy: 0.9074 - val_loss: 7.9613 - val_accuracy: 0.0701 - val_sparse_categorical_accuracy: 0.0701\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.3632 - accuracy: 0.9124 - sparse_categorical_accuracy: 0.9124 - val_loss: 7.2386 - val_accuracy: 0.0715 - val_sparse_categorical_accuracy: 0.0715\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.3320 - accuracy: 0.9166 - sparse_categorical_accuracy: 0.9166 - val_loss: 8.5418 - val_accuracy: 0.0695 - val_sparse_categorical_accuracy: 0.0695\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.3244 - accuracy: 0.9201 - sparse_categorical_accuracy: 0.9201 - val_loss: 8.4597 - val_accuracy: 0.0653 - val_sparse_categorical_accuracy: 0.0653\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.3046 - accuracy: 0.9256 - sparse_categorical_accuracy: 0.9256 - val_loss: 7.4665 - val_accuracy: 0.0690 - val_sparse_categorical_accuracy: 0.0690\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.2760 - accuracy: 0.9321 - sparse_categorical_accuracy: 0.9321 - val_loss: 7.6433 - val_accuracy: 0.0788 - val_sparse_categorical_accuracy: 0.0788\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2754 - accuracy: 0.9324 - sparse_categorical_accuracy: 0.9324 - val_loss: 8.3506 - val_accuracy: 0.0781 - val_sparse_categorical_accuracy: 0.0781\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.3000 - accuracy: 0.9295 - sparse_categorical_accuracy: 0.9295 - val_loss: 8.7063 - val_accuracy: 0.0719 - val_sparse_categorical_accuracy: 0.0719\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.2759 - accuracy: 0.9349 - sparse_categorical_accuracy: 0.9349 - val_loss: 7.6399 - val_accuracy: 0.0773 - val_sparse_categorical_accuracy: 0.0773\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.2438 - accuracy: 0.9419 - sparse_categorical_accuracy: 0.9419 - val_loss: 7.8155 - val_accuracy: 0.0650 - val_sparse_categorical_accuracy: 0.0650\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.2228 - accuracy: 0.9450 - sparse_categorical_accuracy: 0.9450 - val_loss: 7.5218 - val_accuracy: 0.0707 - val_sparse_categorical_accuracy: 0.0707\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.2387 - accuracy: 0.9451 - sparse_categorical_accuracy: 0.9451 - val_loss: 8.6124 - val_accuracy: 0.0775 - val_sparse_categorical_accuracy: 0.0775\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2629 - accuracy: 0.9411 - sparse_categorical_accuracy: 0.9411 - val_loss: 9.0915 - val_accuracy: 0.0756 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2553 - accuracy: 0.9404 - sparse_categorical_accuracy: 0.9404 - val_loss: 8.4649 - val_accuracy: 0.0650 - val_sparse_categorical_accuracy: 0.0650\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.2158 - accuracy: 0.9485 - sparse_categorical_accuracy: 0.9485 - val_loss: 8.5966 - val_accuracy: 0.0802 - val_sparse_categorical_accuracy: 0.0802\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.2475 - accuracy: 0.9461 - sparse_categorical_accuracy: 0.9461 - val_loss: 8.1341 - val_accuracy: 0.0712 - val_sparse_categorical_accuracy: 0.0712\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2077 - accuracy: 0.9502 - sparse_categorical_accuracy: 0.9502 - val_loss: 9.0544 - val_accuracy: 0.0769 - val_sparse_categorical_accuracy: 0.0769\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2326 - accuracy: 0.9479 - sparse_categorical_accuracy: 0.9479 - val_loss: 8.5890 - val_accuracy: 0.0777 - val_sparse_categorical_accuracy: 0.0777\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2202 - accuracy: 0.9503 - sparse_categorical_accuracy: 0.9503 - val_loss: 7.9552 - val_accuracy: 0.0831 - val_sparse_categorical_accuracy: 0.0831\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1827 - accuracy: 0.9557 - sparse_categorical_accuracy: 0.9557 - val_loss: 9.1917 - val_accuracy: 0.0754 - val_sparse_categorical_accuracy: 0.0754\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1806 - accuracy: 0.9574 - sparse_categorical_accuracy: 0.9574 - val_loss: 9.0460 - val_accuracy: 0.0809 - val_sparse_categorical_accuracy: 0.0809\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 88s 159ms/step - loss: 0.2055 - accuracy: 0.9550 - sparse_categorical_accuracy: 0.9550 - val_loss: 8.3881 - val_accuracy: 0.0834 - val_sparse_categorical_accuracy: 0.0834\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2204 - accuracy: 0.9528 - sparse_categorical_accuracy: 0.9528 - val_loss: 8.2924 - val_accuracy: 0.0782 - val_sparse_categorical_accuracy: 0.0782\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 89s 160ms/step - loss: 0.1609 - accuracy: 0.9619 - sparse_categorical_accuracy: 0.9619 - val_loss: 9.0148 - val_accuracy: 0.0849 - val_sparse_categorical_accuracy: 0.0849\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1814 - accuracy: 0.9595 - sparse_categorical_accuracy: 0.9595 - val_loss: 9.1566 - val_accuracy: 0.0830 - val_sparse_categorical_accuracy: 0.0830\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1911 - accuracy: 0.9586 - sparse_categorical_accuracy: 0.9586 - val_loss: 8.1942 - val_accuracy: 0.0774 - val_sparse_categorical_accuracy: 0.0774\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1665 - accuracy: 0.9628 - sparse_categorical_accuracy: 0.9628 - val_loss: 8.6779 - val_accuracy: 0.0724 - val_sparse_categorical_accuracy: 0.0724\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 87s 157ms/step - loss: 0.1851 - accuracy: 0.9598 - sparse_categorical_accuracy: 0.9598 - val_loss: 8.0348 - val_accuracy: 0.0896 - val_sparse_categorical_accuracy: 0.0896\n",
      "Epoch 57/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.1631 - accuracy: 0.9650 - sparse_categorical_accuracy: 0.9650 - val_loss: 9.4740 - val_accuracy: 0.0749 - val_sparse_categorical_accuracy: 0.0749\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.1853 - accuracy: 0.9603 - sparse_categorical_accuracy: 0.9603 - val_loss: 9.5029 - val_accuracy: 0.0859 - val_sparse_categorical_accuracy: 0.0859\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 89s 159ms/step - loss: 0.1752 - accuracy: 0.9623 - sparse_categorical_accuracy: 0.9623 - val_loss: 9.5427 - val_accuracy: 0.0948 - val_sparse_categorical_accuracy: 0.0948\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1781 - accuracy: 0.9646 - sparse_categorical_accuracy: 0.9646 - val_loss: 9.2105 - val_accuracy: 0.0803 - val_sparse_categorical_accuracy: 0.0803\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 88s 157ms/step - loss: 0.1533 - accuracy: 0.9660 - sparse_categorical_accuracy: 0.9660 - val_loss: 8.3504 - val_accuracy: 0.0899 - val_sparse_categorical_accuracy: 0.0899\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 87s 156ms/step - loss: 0.1527 - accuracy: 0.9670 - sparse_categorical_accuracy: 0.9670 - val_loss: 8.7463 - val_accuracy: 0.0766 - val_sparse_categorical_accuracy: 0.0766\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1525 - accuracy: 0.9680 - sparse_categorical_accuracy: 0.9680 - val_loss: 9.9085 - val_accuracy: 0.0844 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1567 - accuracy: 0.9679 - sparse_categorical_accuracy: 0.9679 - val_loss: 9.0972 - val_accuracy: 0.0783 - val_sparse_categorical_accuracy: 0.0783\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.1341 - accuracy: 0.9695 - sparse_categorical_accuracy: 0.9695 - val_loss: 9.0182 - val_accuracy: 0.0756 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1788 - accuracy: 0.9644 - sparse_categorical_accuracy: 0.9644 - val_loss: 9.5578 - val_accuracy: 0.0749 - val_sparse_categorical_accuracy: 0.0749\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1618 - accuracy: 0.9662 - sparse_categorical_accuracy: 0.9662 - val_loss: 11.4209 - val_accuracy: 0.0696 - val_sparse_categorical_accuracy: 0.0696\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1433 - accuracy: 0.9687 - sparse_categorical_accuracy: 0.9687 - val_loss: 11.4126 - val_accuracy: 0.0798 - val_sparse_categorical_accuracy: 0.0798\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1469 - accuracy: 0.9711 - sparse_categorical_accuracy: 0.9711 - val_loss: 10.6431 - val_accuracy: 0.0740 - val_sparse_categorical_accuracy: 0.0740\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1412 - accuracy: 0.9695 - sparse_categorical_accuracy: 0.9695 - val_loss: 10.4324 - val_accuracy: 0.0732 - val_sparse_categorical_accuracy: 0.0732\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1253 - accuracy: 0.9742 - sparse_categorical_accuracy: 0.9742 - val_loss: 11.4619 - val_accuracy: 0.0659 - val_sparse_categorical_accuracy: 0.0659\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1657 - accuracy: 0.9691 - sparse_categorical_accuracy: 0.9691 - val_loss: 9.9297 - val_accuracy: 0.0718 - val_sparse_categorical_accuracy: 0.0718\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1321 - accuracy: 0.9729 - sparse_categorical_accuracy: 0.9729 - val_loss: 10.6403 - val_accuracy: 0.0712 - val_sparse_categorical_accuracy: 0.0712\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1506 - accuracy: 0.9703 - sparse_categorical_accuracy: 0.9703 - val_loss: 10.6331 - val_accuracy: 0.0779 - val_sparse_categorical_accuracy: 0.0779\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1510 - accuracy: 0.9709 - sparse_categorical_accuracy: 0.9709 - val_loss: 10.9636 - val_accuracy: 0.0688 - val_sparse_categorical_accuracy: 0.0688\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1564 - accuracy: 0.9693 - sparse_categorical_accuracy: 0.9693 - val_loss: 10.9047 - val_accuracy: 0.0696 - val_sparse_categorical_accuracy: 0.0696\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1389 - accuracy: 0.9722 - sparse_categorical_accuracy: 0.9722 - val_loss: 11.2284 - val_accuracy: 0.0743 - val_sparse_categorical_accuracy: 0.0743\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1634 - accuracy: 0.9701 - sparse_categorical_accuracy: 0.9701 - val_loss: 13.5056 - val_accuracy: 0.0667 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1658 - accuracy: 0.9712 - sparse_categorical_accuracy: 0.9712 - val_loss: 9.4186 - val_accuracy: 0.0700 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1223 - accuracy: 0.9752 - sparse_categorical_accuracy: 0.9752 - val_loss: 11.2477 - val_accuracy: 0.0564 - val_sparse_categorical_accuracy: 0.0564\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1297 - accuracy: 0.9740 - sparse_categorical_accuracy: 0.9740 - val_loss: 13.0744 - val_accuracy: 0.0721 - val_sparse_categorical_accuracy: 0.0721\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1614 - accuracy: 0.9712 - sparse_categorical_accuracy: 0.9712 - val_loss: 13.6691 - val_accuracy: 0.0706 - val_sparse_categorical_accuracy: 0.0706\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1432 - accuracy: 0.9724 - sparse_categorical_accuracy: 0.9724 - val_loss: 14.6713 - val_accuracy: 0.0700 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2005 - accuracy: 0.9667 - sparse_categorical_accuracy: 0.9667 - val_loss: 17.9927 - val_accuracy: 0.0775 - val_sparse_categorical_accuracy: 0.0775\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1769 - accuracy: 0.9720 - sparse_categorical_accuracy: 0.9720 - val_loss: 12.9085 - val_accuracy: 0.0728 - val_sparse_categorical_accuracy: 0.0728\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1660 - accuracy: 0.9731 - sparse_categorical_accuracy: 0.9731 - val_loss: 11.0312 - val_accuracy: 0.0669 - val_sparse_categorical_accuracy: 0.0669\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1532 - accuracy: 0.9729 - sparse_categorical_accuracy: 0.9729 - val_loss: 11.1144 - val_accuracy: 0.0681 - val_sparse_categorical_accuracy: 0.0681\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1394 - accuracy: 0.9742 - sparse_categorical_accuracy: 0.9742 - val_loss: 13.9408 - val_accuracy: 0.0696 - val_sparse_categorical_accuracy: 0.0696\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1677 - accuracy: 0.9724 - sparse_categorical_accuracy: 0.9724 - val_loss: 13.6590 - val_accuracy: 0.0751 - val_sparse_categorical_accuracy: 0.0751\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.1220 - accuracy: 0.9767 - sparse_categorical_accuracy: 0.9767 - val_loss: 11.1723 - val_accuracy: 0.0868 - val_sparse_categorical_accuracy: 0.0868\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1447 - accuracy: 0.9742 - sparse_categorical_accuracy: 0.9742 - val_loss: 16.1366 - val_accuracy: 0.0843 - val_sparse_categorical_accuracy: 0.0843\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.1744 - accuracy: 0.9714 - sparse_categorical_accuracy: 0.9714 - val_loss: 13.4691 - val_accuracy: 0.0725 - val_sparse_categorical_accuracy: 0.0725\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 85s 152ms/step - loss: 0.2003 - accuracy: 0.9704 - sparse_categorical_accuracy: 0.9704 - val_loss: 10.8978 - val_accuracy: 0.0725 - val_sparse_categorical_accuracy: 0.0725\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.1414 - accuracy: 0.9765 - sparse_categorical_accuracy: 0.9765 - val_loss: 11.2833 - val_accuracy: 0.0821 - val_sparse_categorical_accuracy: 0.0821\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 84s 151ms/step - loss: 0.1456 - accuracy: 0.9752 - sparse_categorical_accuracy: 0.9752 - val_loss: 13.5327 - val_accuracy: 0.0777 - val_sparse_categorical_accuracy: 0.0777\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 84s 152ms/step - loss: 0.1413 - accuracy: 0.9758 - sparse_categorical_accuracy: 0.9758 - val_loss: 15.3932 - val_accuracy: 0.0700 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.2000 - accuracy: 0.9712 - sparse_categorical_accuracy: 0.9712 - val_loss: 12.9620 - val_accuracy: 0.0798 - val_sparse_categorical_accuracy: 0.0798\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1599 - accuracy: 0.9745 - sparse_categorical_accuracy: 0.9745 - val_loss: 13.8767 - val_accuracy: 0.0815 - val_sparse_categorical_accuracy: 0.0815\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1443 - accuracy: 0.9762 - sparse_categorical_accuracy: 0.9762 - val_loss: 16.7640 - val_accuracy: 0.0701 - val_sparse_categorical_accuracy: 0.0701\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 85s 153ms/step - loss: 0.1483 - accuracy: 0.9759 - sparse_categorical_accuracy: 0.9759 - val_loss: 13.7429 - val_accuracy: 0.0662 - val_sparse_categorical_accuracy: 0.0662\n"
     ]
    }
   ],
   "source": [
    "history_cnn_spatial_stream = model_spat.fit(train_ds, \n",
    "                                            validation_data = val_ds, \n",
    "                                            validation_steps=step_per_epoch_val, \n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs, \n",
    "                                            steps_per_epoch=step_per_epoch_train, \n",
    "                                            callbacks=[model_checkpoint_callback, early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c0f3e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 69s 282ms/step - loss: 13.7399 - accuracy: 0.0662 - sparse_categorical_accuracy: 0.0662\n",
      "Loss val: 13.739862442016602\n",
      "Sparse_categorical_accuracy: 0.06616108864545822\n"
     ]
    }
   ],
   "source": [
    "score = model_spat.evaluate(val_ds, verbose=1, batch_size = batch_size, steps=step_per_epoch_val)\n",
    "print('Loss val:', score[0])\n",
    "print('Sparse_categorical_accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36a163",
   "metadata": {},
   "source": [
    "### History e grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc05efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./Models/spatial_stream/spatial_stream_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}.npy',\n",
    "        history_cnn_spatial_stream.history)\n",
    "#history=np.load('my_history.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_cnn_spatial_stream.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ffaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "metric_plot = 'Accuracy'\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/motion_stream/motion_stream_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_top_k_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_top_k_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Top 5 Model Accuracy')\n",
    "metric_plot = 'Top 5 Categorical_accuracy'\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/motion_stream/motion_stream_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb62ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['loss'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_loss'])\n",
    "#plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Loss')\n",
    "metric_plot = 'loss'\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/motion_stream/motion_stream_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_cnn_spatial_stream.history).plot(figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spat.save(f'./Models/spatial_stream/spatial_stream_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}.h5')\n",
    "# model_mot.save_weights('./Models/model_mot_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef9dc9",
   "metadata": {},
   "source": [
    "### Modello Migliore Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059504f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_motion = keras.models.load_model('./Models/motion_model_v4_a3_val_acc_best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78f5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 109, 109, 96)      14208     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 109, 109, 96)     384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 57, 57, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " zero_padding2d_2 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " zero_padding2d_3 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 51)                104499    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,789,747\n",
      "Trainable params: 117,789,043\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "score = best_model_motion.evaluate(val_ds, verbose=1, batch_size = batch_size, steps=step_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28616d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model_spat, to_file='./Models/model_spat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e4c75e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig('./Plots/resnet1_0.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8999322",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_cnn_spatial_stream.history['loss'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_loss'])\n",
    "plt.ylim([0.0, 9.0])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'valid'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cnn_spatial_stream.history['accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_accuracy'])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['train', 'valid'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DataExploration.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deepL37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5f73b1806a9ec290aebb7f16dd5394fd2c9a0b8bec90a8791426b7ef5a3d9604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
