{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff3c55f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe10128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.core.display import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from keras.layers.core import Dense,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4a195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Desktop\\deep-learning-video-classification\n"
     ]
    }
   ],
   "source": [
    "#path dove si trova il dataframe hmdb51\n",
    "path = Path.cwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c48d39",
   "metadata": {},
   "source": [
    "Testing if cuda is on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bff241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802b407",
   "metadata": {},
   "source": [
    "# Spatial Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/hmdb51'\n",
    "path_rowframes = './data/hmdb51/rawframes/'\n",
    "path_annotations = './data/hmdb51/annotations/'\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 128\n",
    "num_classes = 51\n",
    "\n",
    "num_frames_desired = 17     #number of frames per clip\n",
    "type_frame = 'img'          #img / flow_x / flow_y\n",
    "epochs = 200\n",
    "# partition = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dc9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  # layers.Resizing(img_height, img_height),\n",
    "  tf.keras.layers.Rescaling(1./127.5, offset=-1),\n",
    "  #tf.keras.layers.Resizing(img_height, img_width)\n",
    "])\n",
    "\n",
    "resize = tf.keras.Sequential([\n",
    "  tf.keras.layers.Resizing(img_height, img_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e047eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.20, fill_mode = \"nearest\"),\n",
    "    # layers.RandomZoom(0.2),\n",
    "    layers.RandomCrop(img_height,img_width),\n",
    "    # layers.RandomHeight(0.2),\n",
    "    # layers.RandomWidth(0.2),\n",
    "    # layers.Resizing(224,224)\n",
    "    layers.Resizing(img_height, img_width)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb40a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(list, num_frames_desired):\n",
    "    step = len(list) // (num_frames_desired)\n",
    "    #selected_frames = list(range(0, len(list), step))[:num_frames_desired]\n",
    "    sampled_list = list[0:len(list):step][:num_frames_desired]\n",
    "    return(sampled_list)\n",
    "\n",
    "def parse_image(filename):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # image = tf.image.random_flip_left_right(image)\n",
    "    # image = tf.image.resize(image, [img_height, img_width])\n",
    "    return image\n",
    "\n",
    "\"\"\"def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat() # repeat determines how many times the samples can be repeated in the given dataset. (def = 0)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds\"\"\"\n",
    "    \n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.cache()\n",
    "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000)\n",
    "  if augment:\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  else: \n",
    "    ds = ds.map(lambda x, y: (resize(x), y), \n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  # Batch all datasets.\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.repeat() # repeat determines how many times the samples can be repeated in the given dataset. (def = 0)\n",
    "  # Use data augmentation only on the training set.\n",
    "  #if augment:\n",
    "  #  ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
    "  #              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def find_paths(partition, type_frame, num_frames_desired):\n",
    "    if partition == 'train':\n",
    "        video_list = pd.read_csv(f'{path_annotations}/hmdb51_train_split_1_rawframes.txt', sep=\" \", header=None) #train\n",
    "        video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "    elif partition == 'val':\n",
    "        video_list = pd.read_csv(f'{path_annotations}/hmdb51_val_split_1_rawframes.txt', sep=\" \", header=None) #test\n",
    "        video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "    else:\n",
    "        raise Exception(\"invalid partition\")\n",
    "\n",
    "    #temp_path = video_list.loc[0]['path'] #da togliere!!!\n",
    "\n",
    "    paths = []\n",
    "    classes = []\n",
    "    for index, row in video_list.iterrows(): #da togliere [:1]\n",
    "        temp_path = row['path']                    #da rimuovere il commentato\n",
    "        frame_list = os.listdir(os.path.join(f'./{temp_path}'))\n",
    "\n",
    "        frame_list_type = [i for i in frame_list if i.startswith(f'{type_frame}')]\n",
    "\n",
    "        filename = sampling(frame_list_type, num_frames_desired)\n",
    "\n",
    "        paths.extend([os.path.join('.\\\\', temp_path, file) for file in filename])\n",
    "        temp = [row['class']] * num_frames_desired\n",
    "        classes.extend(temp)\n",
    "\n",
    "    #return(list(zip(paths, classes)))\n",
    "    return(list(zip(paths, classes)), video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6384ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- create train set\n",
    "filenames, video_list = find_paths(partition='train', type_frame=type_frame, num_frames_desired=num_frames_desired)\n",
    "video_list[['cose', 'class', 'vid']] = video_list[\"path\"].str.rsplit(\"\\\\\", n = 2, expand = True)\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "zipped = [list(t) for t in zip(*filenames)]\n",
    "\n",
    "filenames = zipped[0]\n",
    "labels = zipped[1]\n",
    "\n",
    "filenames_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "images_ds = filenames_ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "# train_ds = configure_for_performance(ds)\n",
    "train_ds = prepare(ds, shuffle=True, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71781ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60690\n"
     ]
    }
   ],
   "source": [
    "frame_number_train = len(filenames)\n",
    "print(frame_number_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a40fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- create val test\n",
    "filenames, video_list = find_paths(partition='val', type_frame=type_frame, num_frames_desired=num_frames_desired)\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "zipped = [list(t) for t in zip(*filenames)]\n",
    "\n",
    "filenames = zipped[0]\n",
    "labels = zipped[1]\n",
    "\n",
    "filenames_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "images_ds = filenames_ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "# val_ds = configure_for_performance(ds)\n",
    "val_ds = prepare(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdd738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n"
     ]
    }
   ],
   "source": [
    "frame_number_val = len(filenames)\n",
    "print(frame_number_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf13314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "step_per_epoch_train = frame_number_train // batch_size\n",
    "step_per_epoch_val = frame_number_val // batch_size\n",
    "print(step_per_epoch_train)\n",
    "print(step_per_epoch_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c56de",
   "metadata": {},
   "source": [
    "# Spatial Stream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "version = \"v4\"\n",
    "arc = \"a3\"\n",
    "checkpoint_filepath = f'./Models/spatial_stream_CNN/{version}/spatial_model_{version}_{arc}_val_acc_best.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3abfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def prep_fn(img):\n",
    "    img = img / 255.0\n",
    "    img = (img - 0.5) * 2\n",
    "    return img\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    if width >= dy:\n",
    "        top = int((height - dy) * random.random())\n",
    "        left = int((width - dx) * random.random())\n",
    "        right = left + width\n",
    "        bottom = top + height\n",
    "    else: \n",
    "        top = int((height - dy) * random.random())\n",
    "        bottom = top + height\n",
    "        left = 0\n",
    "        right = width\n",
    "    img = img[top : bottom, left : right] \n",
    "    return img\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    preprocessing_function=prep_fn,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20, fill_mode='nearest'\n",
    "    \n",
    ") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54af4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    # layers.RandomZoom(0.2),\n",
    "    layers.RandomCrop(224,224)\n",
    "    # layers.RandomHeight(0.2),\n",
    "    # layers.RandomWidth(0.2),\n",
    "    # layers.Resizing(224,224)\n",
    "])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a260b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 51\n",
    "\n",
    "# model_spat = keras.models.Sequential(tf.keras.layers.Rescaling(1./127.5, offset=-1))\n",
    "model_spat = keras.models.Sequential()\n",
    "\n",
    "#data_augmentation\n",
    "\n",
    "model_spat.add(keras.layers.Conv2D(96, (7,7), strides = 2, input_shape=(224, 224, 3), activation = \"relu\"))\n",
    "model_spat.add(keras.layers.BatchNormalization())\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(256, (5,5), strides = 2, activation='relu'))\n",
    "model_spat.add(keras.layers.BatchNormalization())\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "          \n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_spat.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_spat.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "model_spat.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_spat.add(keras.layers.Flatten())\n",
    "\n",
    "model_spat.add(keras.layers.Dense(2048, activation='relu'))\n",
    "model_spat.add(keras.layers.Dropout(0.5))      #valore dropout 0.5 oppure 0.9? paper li usa entrambi \n",
    "\n",
    "model_spat.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model_spat.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model_spat.add(keras.layers.Dense(num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15246a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 109, 109, 96)      14208     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 109, 109, 96)     384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 57, 57, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " zero_padding2d_2 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " zero_padding2d_3 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 51)                104499    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,789,747\n",
      "Trainable params: 117,789,043\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_spat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c12682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot architettura\n",
    "plot_model(model_spat, to_file=f'./Report/Plots/spatial_stream_CNN/{version}/spatial_stream_schema{version}_{arc}.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e69ae4",
   "metadata": {
    "id": "950cdb45"
   },
   "outputs": [],
   "source": [
    "#model_spat.compile(loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy','sparse_categorical_accuracy'], \n",
    "#              optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "learning_rate = 1e-3 # -2\n",
    "momentum = 0.9\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model_spat.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                   metrics=['sparse_categorical_accuracy','sparse_top_k_categorical_accuracy'], \n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed9a1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "step_per_epoch_train = frame_number_train // batch_size\n",
    "step_per_epoch_val = frame_number_val // batch_size\n",
    "print(step_per_epoch_train)\n",
    "print(step_per_epoch_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc456a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34d7d98d",
    "outputId": "e39004e4-5c77-4d59-b9b7-054d2a29254f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "474/474 [==============================] - 504s 1s/step - loss: 3.7912 - sparse_categorical_accuracy: 0.0500 - sparse_top_k_categorical_accuracy: 0.2102 - val_loss: 3.7064 - val_sparse_categorical_accuracy: 0.0677 - val_sparse_top_k_categorical_accuracy: 0.2368\n",
      "Epoch 2/200\n",
      "260/474 [===============>..............] - ETA: 8:36 - loss: 3.4032 - sparse_categorical_accuracy: 0.1128 - sparse_top_k_categorical_accuracy: 0.3587"
     ]
    }
   ],
   "source": [
    "history_cnn_spatial_stream = model_spat.fit(train_ds, \n",
    "                                            validation_data = val_ds, \n",
    "                                            validation_steps=step_per_epoch_val, \n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs, \n",
    "                                            steps_per_epoch=step_per_epoch_train, \n",
    "                                            callbacks=[model_checkpoint_callback, early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f3e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22/203 [==>...........................] - ETA: 13:58 - loss: 3.6517 - sparse_categorical_accuracy: 0.1033 - sparse_top_k_categorical_accuracy: 0.3132"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22420\\1080642022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_spat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_per_epoch_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss val:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sparse_categorical_accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2957\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1854\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = model_spat.evaluate(val_ds, verbose=1, batch_size = batch_size, steps=step_per_epoch_val)\n",
    "print('Loss val:', score[0])\n",
    "print('Sparse_categorical_accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36a163",
   "metadata": {},
   "source": [
    "### History e grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc05efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./Models/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}.npy',\n",
    "        history_cnn_spatial_stream.history)\n",
    "#history=np.load('my_history.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_cnn_spatial_stream.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ffaa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_cnn_spatial_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4000\\3320068431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_cnn_spatial_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sparse_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_cnn_spatial_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_sparse_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mymax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_cnn_spatial_stream' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Sparse Categorical Accuracy')\n",
    "metric_plot = 'Accuracy'\n",
    "plt.ylabel('Sparse Categorical Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_top_k_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_top_k_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Sparse Top 5 Categorical Accuracy')\n",
    "metric_plot = 'Top 5 Categorical_accuracy'\n",
    "plt.ylabel('Sparse Top 5 Categorical Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb62ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['loss'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_loss'])\n",
    "#plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Loss (sparse categorical crossentropy)')\n",
    "metric_plot = 'loss'\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig(f'./Report/Plots/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}_metric{metric_plot}_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_cnn_spatial_stream.history).plot(figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spat.save(f'./Models/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_epoch{epochs}_batch{batch_size}_optSDG_{learning_rate}_{momentum}.h5')\n",
    "# model_mot.save_weights('./Models/model_mot_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef9dc9",
   "metadata": {},
   "source": [
    "### Modello Migliore Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059504f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_motion = keras.models.load_model(f'./Models/spatial_stream_CNN/{version}/spatial_stream_CNN_{version}_{arc}_val_acc_best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78f5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 109, 109, 96)      14208     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 109, 109, 96)     384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 57, 57, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " zero_padding2d_2 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " zero_padding2d_3 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 51)                104499    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,789,747\n",
      "Trainable params: 117,789,043\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "score = best_model_motion.evaluate(val_ds, verbose=1, batch_size = batch_size, steps=step_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28616d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model_spat, to_file='./Models/model_spat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e4c75e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig('./Plots/resnet1_0.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8999322",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_cnn_spatial_stream.history['loss'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_loss'])\n",
    "plt.ylim([0.0, 9.0])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'valid'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cnn_spatial_stream.history['accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_accuracy'])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['train', 'valid'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DataExploration.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deepL37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5f73b1806a9ec290aebb7f16dd5394fd2c9a0b8bec90a8791426b7ef5a3d9604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
