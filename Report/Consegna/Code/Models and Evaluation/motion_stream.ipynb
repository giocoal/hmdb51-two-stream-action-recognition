{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from keras.layers.core import Dense,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "path = './data/hmdb51'\n",
    "path_rowframes = './data/hmdb51/rawframes/'\n",
    "path_annotations = './data/hmdb51/annotations/'\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Stream Model: Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './Models/motion_model{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dichiarazione architettura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 51\n",
    "\n",
    "model_mot= keras.models.Sequential()\n",
    "\n",
    "# data_augmentation\n",
    "\n",
    "model_mot.add(keras.layers.Conv2D(96, (7,7), strides = 2, input_shape=(224, 224, 20), activation = \"relu\"))\n",
    "model_mot.add(keras.layers.BatchNormalization())\n",
    "model_mot.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_mot.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_mot.add(keras.layers.Conv2D(256, (5,5), strides = 2, activation='relu'))\n",
    "model_mot.add(keras.layers.BatchNormalization())\n",
    "model_mot.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "          \n",
    "model_mot.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_mot.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_mot.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_mot.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "\n",
    "model_mot.add(keras.layers.ZeroPadding2D(padding = (1,1)))\n",
    "model_mot.add(keras.layers.Conv2D(512, (3,3), strides = 1, activation='relu'))\n",
    "model_mot.add(keras.layers.MaxPooling2D((3,3), strides=2, padding=\"same\"))\n",
    "\n",
    "model_mot.add(keras.layers.Flatten())\n",
    "\n",
    "model_mot.add(keras.layers.Dense(4096, activation='relu'))\n",
    "model_mot.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model_mot.add(keras.layers.Dense(2048, activation='relu'))\n",
    "model_mot.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model_mot.add(keras.layers.Dense(num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 109, 109, 96)      94176     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 109, 109, 96)     384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 57, 57, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " zero_padding2d_2 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " zero_padding2d_3 (ZeroPaddi  (None, 16, 16, 512)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 51)                104499    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,869,715\n",
      "Trainable params: 117,869,011\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mot.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9, nesterov=True)\n",
    "# keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_mot.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                   metrics=['sparse_categorical_accuracy','sparse_top_k_categorical_accuracy'], \n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generator Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "path = './data/hmdb51'\n",
    "path_rowframes = './data/hmdb51/rawframes/'\n",
    "path_annotations = './data/hmdb51/annotations/'\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "num_classes = 51\n",
    "\n",
    "# Parametri del batch generator\n",
    "num_of_snip=1\n",
    "opt_flow_len=10\n",
    "image_shape=(img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, \n",
    "                 num_of_snip=1, \n",
    "                 opt_flow_len=10, \n",
    "                 image_shape=(224, 224),\n",
    "                 partition='train'):\n",
    "\n",
    "        self.opt_flow_len = opt_flow_len\n",
    "        self.num_of_snip = num_of_snip\n",
    "        self.image_shape = image_shape\n",
    "        self.opt_flow_path = os.path.join(path_rowframes)\n",
    "        self.path_annotations = path_annotations\n",
    "        self.partition = partition\n",
    "        \n",
    "        # Get data\n",
    "        self.video_list = self.find_videos_and_metadata()\n",
    "        \n",
    "    def find_videos_and_metadata(self):\n",
    "        if self.partition == 'train':\n",
    "            video_list = pd.read_csv(f'{self.path_annotations}/hmdb51_train_split_1_rawframes.txt', sep=\" \", header=None) #train\n",
    "            video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "        elif self.partition == 'val':\n",
    "            video_list = pd.read_csv(f'{self.path_annotations}/hmdb51_val_split_1_rawframes.txt', sep=\" \", header=None) #test\n",
    "            video_list.columns = [\"path\", \"num_frames_tot\", \"class\"]\n",
    "        else:\n",
    "            raise Exception(\"invalid partition\")\n",
    "        return(video_list)\n",
    "    \n",
    "    def generator(self, batch_size):\n",
    "        video_list = self.video_list\n",
    "        idx = 0\n",
    "        print(f\"Creating {self.partition} with {len(self.video_list)} samples.\")\n",
    "        while 1:\n",
    "            idx +=1\n",
    "            X, y = [], []\n",
    "            for _ in range(batch_size):\n",
    "                # Reset \n",
    "                stack = []\n",
    "\n",
    "                # Get a random sample.\n",
    "                row = video_list.sample(n=1).values.tolist()\n",
    "\n",
    "                # Get the stacked optical flows from disk.\n",
    "                stack = self.find_stacked_optical_flows(row)\n",
    "\n",
    "                X.append(stack)\n",
    "                y.append(row[0][2])\n",
    "\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            y = np.squeeze(y)\n",
    "\n",
    "            yield X, y\n",
    "        \n",
    "    def generator1(self, batch_size):\n",
    "        video_list = self.video_list\n",
    "        idx = 0\n",
    "        print(f\"Creating {self.partition} with {len(self.video_list)} samples.\")\n",
    "        idx +=1\n",
    "        X, y = [], []\n",
    "        for _ in range(batch_size):\n",
    "            # Reset \n",
    "            stack = []\n",
    "\n",
    "            # Get a random sample.\n",
    "            row = video_list.sample(n=1).values.tolist()\n",
    "\n",
    "            # Get the stacked optical flows from disk.\n",
    "            stack = self.find_stacked_optical_flows(row)\n",
    "            print(stack)\n",
    "            X.append(stack)\n",
    "            y.append(row[0][2])\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "            \n",
    "    def find_stacked_optical_flows(self, row):\n",
    "        opt_flow_stack = []\n",
    "        opt_flow_dir = self.opt_flow_path\n",
    "        \n",
    "        # Temporal parameters\n",
    "        total_frames = row[0][1]\n",
    "        win_len = (total_frames - self.opt_flow_len) // self.num_of_snip\n",
    "        if self.partition=='train':\n",
    "            start_frame = int(random.random() * win_len) + 1\n",
    "        else:\n",
    "            start_frame = int(0.5 * win_len) + 1\n",
    "        frames = [] # selected optical flow frames\n",
    "        for i in range(self.num_of_snip):\n",
    "            frames += range(start_frame + self.opt_flow_len * i, \n",
    "                            start_frame + self.opt_flow_len * (i + 1))  \n",
    "        if self.partition == 'train' and random.random() > 0.5:\n",
    "            flip = True\n",
    "        else:\n",
    "            flip = False\n",
    "        \n",
    "        # Spatial Parameter\n",
    "        img_test = cv2.imread(os.path.join(f'.\\\\{row[0][0]}' + '\\\\flow_x_' + str(\"%05d\"%(1)) + '.jpg'), 0)\n",
    "        top = int((img_test.shape[0] - self.image_shape[0]) * random.random())\n",
    "        left = int((img_test.shape[1] - self.image_shape[1]) * random.random())\n",
    "        right = left + self.image_shape[1]\n",
    "        bottom = top + self.image_shape[0]\n",
    "        \n",
    "        # loop over frames\n",
    "        for i_frame in frames:\n",
    "            # x flow\n",
    "            img = None \n",
    "            temp_path = None\n",
    "            temp_path = row[0][0]\n",
    "            img = cv2.imread(os.path.join(f'.\\\\{temp_path}' + '\\\\flow_x_' + str(\"%05d\"%(i_frame)) + '.jpg'), 0)\n",
    "            #print(os.path.join(f'.\\\\{temp_path}' + '\\\\flow_x_' + str(\"%05d\"%(i_frame)) + '.jpg'))\n",
    "            #print(img.shape)\n",
    "            img = np.array(img)\n",
    "            # mean substraction \n",
    "            img = img - np.mean(img)\n",
    "            if self.partition == 'train':\n",
    "                # random crop\n",
    "                img = img[top : bottom, left : right]\n",
    "            else:\n",
    "                # resize\n",
    "                img = cv2.resize(img, self.image_shape)\n",
    "            img = img / 255. # normalize pixels \n",
    "            if flip:\n",
    "                img = -img\n",
    "            img = cv2.resize(img, self.image_shape)\n",
    "            #print(img.shape)\n",
    "            opt_flow_stack.append(img)\n",
    "            \n",
    "            # y flow\n",
    "            img2 = None \n",
    "            img2 = cv2.imread(os.path.join(f'.\\\\{temp_path}' + '\\\\flow_y_' + str(\"%05d\"%(i_frame)) + '.jpg'), 0)\n",
    "            #print(img2.shape)\n",
    "            img2 = np.array(img2)\n",
    "            #img2 = np.swapaxes(img2, 0, 1)\n",
    "            img2 = img2 - np.mean(img2)\n",
    "            if self.partition == 'train':\n",
    "                 #random crop\n",
    "                img2 = img2[top : bottom, left : right]\n",
    "            else:\n",
    "                #resize\n",
    "                img2= cv2.resize(img2, self.image_shape)\n",
    "            img2 = img2 / 255. # normalize pixels \n",
    "            if flip:\n",
    "                img2 = -img2\n",
    "            img2 = cv2.resize(img2, self.image_shape)\n",
    "            #print(img2.shape)\n",
    "            opt_flow_stack.append(img2)\n",
    "            \n",
    "        opt_flow_stack = np.array(opt_flow_stack)\n",
    "        opt_flow_stack = np.swapaxes(opt_flow_stack, 0, 1)\n",
    "        opt_flow_stack = np.swapaxes(opt_flow_stack, 1, 2)\n",
    "        # random horizontal flip for training sets\n",
    "        if flip:\n",
    "            opt_flow_stack = np.flip(opt_flow_stack, 0)\n",
    "        return opt_flow_stack\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_train = DataSet(num_of_snip=num_of_snip, opt_flow_len=opt_flow_len, partition='train')\n",
    "videos_val = DataSet(num_of_snip=num_of_snip, opt_flow_len=opt_flow_len, partition='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo degli step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# batch size \n",
    "step_per_epoch_train = len(videos_train.video_list) // batch_size\n",
    "print(step_per_epoch_train)\n",
    "step_per_epoch_val = len(videos_val.video_list) // batch_size\n",
    "print(step_per_epoch_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = videos_train.generator(batch_size)\n",
    "\n",
    "val_generator = videos_val.generator(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit 1 e 2: 20 + 20 epoche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 211s 2s/step - loss: 3.3339 - sparse_categorical_accuracy: 0.1481 - sparse_top_k_categorical_accuracy: 0.3998 - val_loss: 3.4688 - val_sparse_categorical_accuracy: 0.1197 - val_sparse_top_k_categorical_accuracy: 0.3703\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 200s 2s/step - loss: 3.3475 - sparse_categorical_accuracy: 0.1492 - sparse_top_k_categorical_accuracy: 0.4139 - val_loss: 3.7319 - val_sparse_categorical_accuracy: 0.0944 - val_sparse_top_k_categorical_accuracy: 0.3085\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 158s 1s/step - loss: 3.3123 - sparse_categorical_accuracy: 0.1610 - sparse_top_k_categorical_accuracy: 0.4178 - val_loss: 3.5510 - val_sparse_categorical_accuracy: 0.1044 - val_sparse_top_k_categorical_accuracy: 0.3457\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 154s 1s/step - loss: 3.2813 - sparse_categorical_accuracy: 0.1554 - sparse_top_k_categorical_accuracy: 0.4262 - val_loss: 3.4813 - val_sparse_categorical_accuracy: 0.1184 - val_sparse_top_k_categorical_accuracy: 0.3783\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 188s 2s/step - loss: 3.2613 - sparse_categorical_accuracy: 0.1633 - sparse_top_k_categorical_accuracy: 0.4361 - val_loss: 3.6097 - val_sparse_categorical_accuracy: 0.1017 - val_sparse_top_k_categorical_accuracy: 0.3604\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 172s 2s/step - loss: 3.2185 - sparse_categorical_accuracy: 0.1788 - sparse_top_k_categorical_accuracy: 0.4454 - val_loss: 3.4558 - val_sparse_categorical_accuracy: 0.1223 - val_sparse_top_k_categorical_accuracy: 0.3770\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 633s 6s/step - loss: 3.2393 - sparse_categorical_accuracy: 0.1703 - sparse_top_k_categorical_accuracy: 0.4389 - val_loss: 3.3199 - val_sparse_categorical_accuracy: 0.1569 - val_sparse_top_k_categorical_accuracy: 0.4275\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 223s 2s/step - loss: 3.2418 - sparse_categorical_accuracy: 0.1605 - sparse_top_k_categorical_accuracy: 0.4403 - val_loss: 3.4097 - val_sparse_categorical_accuracy: 0.1230 - val_sparse_top_k_categorical_accuracy: 0.3684\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 206s 2s/step - loss: 3.2546 - sparse_categorical_accuracy: 0.1771 - sparse_top_k_categorical_accuracy: 0.4305 - val_loss: 3.6824 - val_sparse_categorical_accuracy: 0.0898 - val_sparse_top_k_categorical_accuracy: 0.3451\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 175s 2s/step - loss: 3.1988 - sparse_categorical_accuracy: 0.1779 - sparse_top_k_categorical_accuracy: 0.4471 - val_loss: 3.4688 - val_sparse_categorical_accuracy: 0.1350 - val_sparse_top_k_categorical_accuracy: 0.3723\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 219s 2s/step - loss: 3.1655 - sparse_categorical_accuracy: 0.1872 - sparse_top_k_categorical_accuracy: 0.4611 - val_loss: 3.4064 - val_sparse_categorical_accuracy: 0.1184 - val_sparse_top_k_categorical_accuracy: 0.3963\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 220s 2s/step - loss: 3.1748 - sparse_categorical_accuracy: 0.1844 - sparse_top_k_categorical_accuracy: 0.4645 - val_loss: 3.4177 - val_sparse_categorical_accuracy: 0.1430 - val_sparse_top_k_categorical_accuracy: 0.3896\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 183s 2s/step - loss: 3.1514 - sparse_categorical_accuracy: 0.1985 - sparse_top_k_categorical_accuracy: 0.4676 - val_loss: 3.4613 - val_sparse_categorical_accuracy: 0.1250 - val_sparse_top_k_categorical_accuracy: 0.3830\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 211s 2s/step - loss: 3.0973 - sparse_categorical_accuracy: 0.2024 - sparse_top_k_categorical_accuracy: 0.4837 - val_loss: 3.5030 - val_sparse_categorical_accuracy: 0.1297 - val_sparse_top_k_categorical_accuracy: 0.3743\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 233s 2s/step - loss: 3.0947 - sparse_categorical_accuracy: 0.2061 - sparse_top_k_categorical_accuracy: 0.4786 - val_loss: 3.3521 - val_sparse_categorical_accuracy: 0.1343 - val_sparse_top_k_categorical_accuracy: 0.4242\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 236s 2s/step - loss: 3.1234 - sparse_categorical_accuracy: 0.1976 - sparse_top_k_categorical_accuracy: 0.4699 - val_loss: 3.3185 - val_sparse_categorical_accuracy: 0.1543 - val_sparse_top_k_categorical_accuracy: 0.4009\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 266s 2s/step - loss: 3.0677 - sparse_categorical_accuracy: 0.2089 - sparse_top_k_categorical_accuracy: 0.4961 - val_loss: 3.4248 - val_sparse_categorical_accuracy: 0.1277 - val_sparse_top_k_categorical_accuracy: 0.3996\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 264s 2s/step - loss: 3.0822 - sparse_categorical_accuracy: 0.2013 - sparse_top_k_categorical_accuracy: 0.4952 - val_loss: 3.2983 - val_sparse_categorical_accuracy: 0.1463 - val_sparse_top_k_categorical_accuracy: 0.4255\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 240s 2s/step - loss: 3.0434 - sparse_categorical_accuracy: 0.2097 - sparse_top_k_categorical_accuracy: 0.4961 - val_loss: 3.7662 - val_sparse_categorical_accuracy: 0.1210 - val_sparse_top_k_categorical_accuracy: 0.3305\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 287s 3s/step - loss: 3.0509 - sparse_categorical_accuracy: 0.2190 - sparse_top_k_categorical_accuracy: 0.5163 - val_loss: 3.2954 - val_sparse_categorical_accuracy: 0.1509 - val_sparse_top_k_categorical_accuracy: 0.4229\n"
     ]
    }
   ],
   "source": [
    "# %tb\n",
    "history_cnn_spatial_stream = model_mot.fit_generator(generator =train_generator, \n",
    "                                            steps_per_epoch=step_per_epoch_train, \n",
    "                                            validation_data=val_generator,\n",
    "                                            validation_steps=step_per_epoch_val, \n",
    "                                            epochs=20,\n",
    "                                            callbacks=[model_checkpoint_callback])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit 2: 10 epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 241s 2s/step - loss: 3.0717 - sparse_categorical_accuracy: 0.2106 - sparse_top_k_categorical_accuracy: 0.4893 - val_loss: 3.7142 - val_sparse_categorical_accuracy: 0.1070 - val_sparse_top_k_categorical_accuracy: 0.3517\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 210s 2s/step - loss: 3.0504 - sparse_categorical_accuracy: 0.2083 - sparse_top_k_categorical_accuracy: 0.5025 - val_loss: 3.2706 - val_sparse_categorical_accuracy: 0.1582 - val_sparse_top_k_categorical_accuracy: 0.4402\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 216s 2s/step - loss: 2.9882 - sparse_categorical_accuracy: 0.2280 - sparse_top_k_categorical_accuracy: 0.5242 - val_loss: 3.1496 - val_sparse_categorical_accuracy: 0.1676 - val_sparse_top_k_categorical_accuracy: 0.4747\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 248s 2s/step - loss: 3.0264 - sparse_categorical_accuracy: 0.2210 - sparse_top_k_categorical_accuracy: 0.5065 - val_loss: 3.3618 - val_sparse_categorical_accuracy: 0.1702 - val_sparse_top_k_categorical_accuracy: 0.4435\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 225s 2s/step - loss: 3.0507 - sparse_categorical_accuracy: 0.2173 - sparse_top_k_categorical_accuracy: 0.4966 - val_loss: 3.3837 - val_sparse_categorical_accuracy: 0.1430 - val_sparse_top_k_categorical_accuracy: 0.4209\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 280s 3s/step - loss: 3.0165 - sparse_categorical_accuracy: 0.2258 - sparse_top_k_categorical_accuracy: 0.5096 - val_loss: 3.6500 - val_sparse_categorical_accuracy: 0.1383 - val_sparse_top_k_categorical_accuracy: 0.3890\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 252s 2s/step - loss: 2.9933 - sparse_categorical_accuracy: 0.2311 - sparse_top_k_categorical_accuracy: 0.5135 - val_loss: 3.2994 - val_sparse_categorical_accuracy: 0.1476 - val_sparse_top_k_categorical_accuracy: 0.4242\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 260s 2s/step - loss: 2.9702 - sparse_categorical_accuracy: 0.2275 - sparse_top_k_categorical_accuracy: 0.5242 - val_loss: 3.4446 - val_sparse_categorical_accuracy: 0.1336 - val_sparse_top_k_categorical_accuracy: 0.4069\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 235s 2s/step - loss: 2.9690 - sparse_categorical_accuracy: 0.2227 - sparse_top_k_categorical_accuracy: 0.5180 - val_loss: 4.0352 - val_sparse_categorical_accuracy: 0.0997 - val_sparse_top_k_categorical_accuracy: 0.3251\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 268s 2s/step - loss: 2.9368 - sparse_categorical_accuracy: 0.2368 - sparse_top_k_categorical_accuracy: 0.5377 - val_loss: 3.5616 - val_sparse_categorical_accuracy: 0.1257 - val_sparse_top_k_categorical_accuracy: 0.4043\n"
     ]
    }
   ],
   "source": [
    "history_cnn_spatial_stream_3 = model_mot.fit_generator(generator =train_generator, \n",
    "                                            steps_per_epoch=step_per_epoch_train, \n",
    "                                            validation_data=val_generator,\n",
    "                                            validation_steps=step_per_epoch_val, \n",
    "                                            epochs=10,\n",
    "                                            callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit 3: 10 epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 210s 2s/step - loss: 2.9076 - sparse_categorical_accuracy: 0.2407 - sparse_top_k_categorical_accuracy: 0.5341 - val_loss: 3.3092 - val_sparse_categorical_accuracy: 0.1589 - val_sparse_top_k_categorical_accuracy: 0.4342\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 200s 2s/step - loss: 2.9169 - sparse_categorical_accuracy: 0.2466 - sparse_top_k_categorical_accuracy: 0.5360 - val_loss: 3.2994 - val_sparse_categorical_accuracy: 0.1596 - val_sparse_top_k_categorical_accuracy: 0.4475\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 206s 2s/step - loss: 2.9126 - sparse_categorical_accuracy: 0.2525 - sparse_top_k_categorical_accuracy: 0.5403 - val_loss: 3.3522 - val_sparse_categorical_accuracy: 0.1416 - val_sparse_top_k_categorical_accuracy: 0.4076\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 194s 2s/step - loss: 2.9135 - sparse_categorical_accuracy: 0.2416 - sparse_top_k_categorical_accuracy: 0.5405 - val_loss: 3.3136 - val_sparse_categorical_accuracy: 0.1656 - val_sparse_top_k_categorical_accuracy: 0.4508\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 226s 2s/step - loss: 2.9175 - sparse_categorical_accuracy: 0.2424 - sparse_top_k_categorical_accuracy: 0.5484 - val_loss: 3.6180 - val_sparse_categorical_accuracy: 0.1243 - val_sparse_top_k_categorical_accuracy: 0.3504\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 201s 2s/step - loss: 2.9118 - sparse_categorical_accuracy: 0.2430 - sparse_top_k_categorical_accuracy: 0.5338 - val_loss: 3.6794 - val_sparse_categorical_accuracy: 0.1210 - val_sparse_top_k_categorical_accuracy: 0.3637\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 206s 2s/step - loss: 2.8746 - sparse_categorical_accuracy: 0.2556 - sparse_top_k_categorical_accuracy: 0.5436 - val_loss: 3.8401 - val_sparse_categorical_accuracy: 0.1390 - val_sparse_top_k_categorical_accuracy: 0.3664\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 164s 1s/step - loss: 2.8802 - sparse_categorical_accuracy: 0.2570 - sparse_top_k_categorical_accuracy: 0.5411 - val_loss: 3.3075 - val_sparse_categorical_accuracy: 0.1676 - val_sparse_top_k_categorical_accuracy: 0.4721\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 162s 1s/step - loss: 2.8611 - sparse_categorical_accuracy: 0.2506 - sparse_top_k_categorical_accuracy: 0.5591 - val_loss: 3.3333 - val_sparse_categorical_accuracy: 0.1622 - val_sparse_top_k_categorical_accuracy: 0.4169\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 157s 1s/step - loss: 2.8721 - sparse_categorical_accuracy: 0.2497 - sparse_top_k_categorical_accuracy: 0.5526 - val_loss: 3.3887 - val_sparse_categorical_accuracy: 0.1782 - val_sparse_top_k_categorical_accuracy: 0.4461\n"
     ]
    }
   ],
   "source": [
    "history_cnn_spatial_stream_4 = model_mot.fit_generator(generator =train_generator, \n",
    "                                            steps_per_epoch=step_per_epoch_train, \n",
    "                                            validation_data=val_generator,\n",
    "                                            validation_steps=step_per_epoch_val, \n",
    "                                            epochs=10,\n",
    "                                            callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\Documents\\venv\\deepL37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 71s 2s/step - loss: 3.4610 - sparse_categorical_accuracy: 0.1529 - sparse_top_k_categorical_accuracy: 0.4242\n",
      "sparse_categorical_accuracy: 3.460975170135498\n",
      "sparse_top_k_categorical_accuracy: 0.1529255360364914\n"
     ]
    }
   ],
   "source": [
    "score = model_mot.evaluate_generator(val_generator, verbose=1, steps=step_per_epoch_val)\n",
    "#print('sparse_categorical_accuracy:', score[0])\n",
    "#print('sparse_top_k_categorical_accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(history_cnn_spatial_stream.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history_cnn_spatial_stream.history['val_sparse_categorical_accuracy'])\n",
    "plt.axis(ymin=0,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.savefig('./Plots/resnet1_0.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mot.save('./Models/model_mot.h5')\n",
    "model_mot.save_weights('./Models/model_mot_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deepL37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f73b1806a9ec290aebb7f16dd5394fd2c9a0b8bec90a8791426b7ef5a3d9604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
